Recent success of deep convolutional networks \citep{lecun1998gradient, imagenet,rcnn,fcn,resnet,maskrcnn} has been coupled with increased requirement of computation resources. In particular, the model size, memory footprint, the number of computation operations (FLOPs) and power usage are major aspects inhibiting the use of deep neural networks in some resource-constrained settings. Those large models can be infeasible to store, and run in real time on embedded systems. To address this issue, many methods have been proposed such as low-rank approximation of weights \citep{lowrank1, lowrank2}, weight quantization \citep{binarynet, xnornet}, knowledge distillation \citep{kd, fitnet} and network pruning \citep{han2015learning,li2016pruning}, among which network pruning has gained notable attention due to their competitive performance and compatibility.

One major branch of network pruning methods is individual weight pruning, and it dates back to Optimal Brain Damage \citep{obd} and Optimal Brain Surgeon \citep{obs}, which prune weights based on Hessian of the loss function. More recently, \cite{han2015learning} proposes to prune network weights with small magnitude, and this technique is further incorporated into the ``Deep Compression'' pipeline \citep{han2015deep} to obtain highly compressed models. \cite{datafree} proposes a data-free algorithm to remove redundant neurons iteratively. \cmt{Network-Trim \citep{trim} prune a trained model layer-wisely, by solving a convex optimization problem.} \citet{vdropout} uses  Variatonal Dropout~\citep{vdropoutorg} to prune redundant weights. \citet{l0sparse} learns sparse networks through $L_0$-norm regularization based on stochastic gate. 
However, one drawback of these \emph{unstructured} pruning methods is that the resulting weight matrices are sparse, which cannot lead to compression and speedup without dedicated hardware/libraries \citep{han2016eie}. 

% \vspace{-5ex}
In contrast, \emph{structured} pruning methods prune at the level of channels or even layers. Since the original convolution structure is still preserved, no dedicated hardware/libraries are required to realize the benefits. Among structured pruning methods, channel pruning is the most popular, since it operates at the most fine-grained level while still fitting in conventional deep learning frameworks. Some heuristic methods include pruning channels based on their corresponding filter weight norm \citep{li2016pruning} and average percentage of zeros in the output \citep{trimming}. Group sparsity is also widely used to smooth the pruning process after training \citep{wen2016learning, gs1, gs2, gs3}. \cite{liu2017learning} and \cite{ye2018rethinking} impose  sparsity constraints on channel-wise scaling factors during training, whose magnitudes are then used for channel pruning. \cite{huang2018data} uses a similar technique to prune coarser structures such as residual blocks. \cite{he2017channel} and \cite{luo2017thinet} minimizes next layer's feature reconstruction error to determine which channels to keep. Similarly, \cite{nisp} optimizes the reconstruction error of the final response layer and propagates a ``importance score'' for each channel. \cite{nvidia} uses Taylor expansion to approximate each channel's influence over the final loss and prune accordingly. \cite{pfa} analyzes the intrinsic correlation within each layer and prune redundant channels.  \cite{chin2018layer} proposes a layer-wise compensate filter pruning algorithm to improve commonly-adopted heuristic pruning metrics. \cite{he2018sfp} proposes to allow pruned filters to recover during the training process. \cite{lin2017runtime, wang2017skipnet} prune certain structures in the network based on the current input.

%random, to prune or not to prune, etc.
Our work is also related to some recent studies on the characteristics of pruning algorithms. \cite{recovering} shows that random channel pruning \citep{compact} can perform on par with a variety of more sophisticated pruning criteria, demonstrating the plasticity of network models. 
In the context of unstructured pruning, The Lottery Ticket Hypothesis \citep{lottery} conjectures that  certain connections together with their randomly initialized weights, can enable a comparable accuracy with the original network when trained in isolation. We provide comparisons between \cite{lottery} and this work in Section \ref{ap:lottery}.
\cite{toprune} shows that training a small-dense model cannot achieve the same accuracy as a pruned large-sparse model with identical memory footprint. In this work, we reveal a different and rather surprising characteristic of structured network pruning methods: fine-tuning the pruned model with inherited weights is not better than training it from scratch; the resulting pruned architectures are more likely to be what brings the benefit.
