\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alvarez \& Salzmann(2016)Alvarez and Salzmann]{gs1}
Jose~M Alvarez and Mathieu Salzmann.
\newblock Learning the number of neurons in deep networks.
\newblock In \emph{NIPS}, 2016.

\bibitem[Anwar \& Sung(2016)Anwar and Sung]{compact}
Sajid Anwar and Wonyong Sung.
\newblock Compact deep convolutional neural networks with coarse pruning.
\newblock \emph{arXiv preprint arXiv:1610.09639}, 2016.

\bibitem[Ba \& Caruana(2014)Ba and Caruana]{deep}
Jimmy Ba and Rich Caruana.
\newblock Do deep nets really need to be deep?
\newblock In \emph{NIPS}, 2014.

\bibitem[Baker et~al.(2017)Baker, Gupta, Naik, and Raskar]{rl2}
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar.
\newblock Designing neural network architectures using reinforcement learning.
\newblock \emph{ICLR}, 2017.

\bibitem[Carreira-Perpin{\'a}n \& Idelbayev(2018)Carreira-Perpin{\'a}n and
  Idelbayev]{carreira2018learning}
Miguel~A Carreira-Perpin{\'a}n and Yerlan Idelbayev.
\newblock ``\uppercase{L}earning-compression'' algorithms for neural net
  pruning.
\newblock In \emph{CVPR}, 2018.

\bibitem[Chin et~al.(2018)Chin, Zhang, and Marculescu]{chin2018layer}
Ting-Wu Chin, Cha Zhang, and Diana Marculescu.
\newblock Layer-compensated pruning for resource-constrained convolutional
  neural networks.
\newblock \emph{arXiv preprint arXiv:1810.00518}, 2018.

\bibitem[Courbariaux et~al.(2016)Courbariaux, Hubara, Soudry, El-Yaniv, and
  Bengio]{binarynet}
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Binarized neural networks: Training deep neural networks with weights
  and activations constrained to+ 1 or-1.
\newblock \emph{arXiv preprint arXiv:1602.02830}, 2016.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Denton et~al.(2014)Denton, Zaremba, Bruna, LeCun, and
  Fergus]{lowrank1}
Emily~L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock In \emph{NIPS}, 2014.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Girshick et~al.(2014)Girshick, Donahue, Darrell, and Malik]{rcnn}
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In \emph{CVPR}, 2014.

\bibitem[Gordon et~al.(2018)Gordon, Eban, Nachum, Chen, Wu, Yang, and
  Choi]{gordon2018morphnet}
Ariel Gordon, Elad Eban, Ofir Nachum, Bo~Chen, Hao Wu, Tien-Ju Yang, and Edward
  Choi.
\newblock Morphnet: Fast \& simple resource-constrained structure learning of
  deep networks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{NIPS}, 2015.

\bibitem[Han et~al.(2016{\natexlab{a}})Han, Liu, Mao, Pu, Pedram, Horowitz, and
  Dally]{han2016eie}
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark~A Horowitz, and
  William~J Dally.
\newblock Eie: efficient inference engine on compressed deep neural network.
\newblock In \emph{Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual
  International Symposium on}, 2016{\natexlab{a}}.

\bibitem[Han et~al.(2016{\natexlab{b}})Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{ICLR}, 2016{\natexlab{b}}.

\bibitem[Hassibi \& Stork(1993)Hassibi and Stork]{obs}
Babak Hassibi and David~G Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In \emph{NIPS}, 1993.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[He et~al.(2017{\natexlab{a}})He, Gkioxari, Doll{\'a}r, and
  Girshick]{maskrcnn}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In \emph{ICCVs}, 2017{\natexlab{a}}.

\bibitem[He et~al.(2018{\natexlab{a}})He, Kang, Dong, Fu, and Yang]{he2018sfp}
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi~Yang.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock In \emph{IJCAI}, 2018{\natexlab{a}}.

\bibitem[He et~al.(2018{\natexlab{b}})He, Kang, Dong, Fu, and Yang]{sfpcode}
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi~Yang.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock \emph{https://github.com/he-y/soft-filter-pruning},
  2018{\natexlab{b}}.

\bibitem[He et~al.(2017{\natexlab{b}})He, Zhang, and Sun]{he2017channel}
Yihui He, Xiangyu Zhang, and Jian Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{ICCV}, 2017{\natexlab{b}}.

\bibitem[He et~al.(2018{\natexlab{c}})He, Lin, Liu, Wang, Li, and Han]{amc}
Yihui He, Ji~Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han.
\newblock Amc: Automl for model compression and acceleration on mobile devices.
\newblock In \emph{ECCV}, 2018{\natexlab{c}}.

\bibitem[Hinton et~al.(2014)Hinton, Vinyals, and Dean]{kd}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{NIPS Workshop}, 2014.

\bibitem[Hu et~al.(2016)Hu, Peng, Tai, and Tang]{trimming}
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.
\newblock Network trimming: A data-driven neuron pruning approach towards
  efficient deep architectures.
\newblock \emph{arXiv preprint arXiv:1607.03250}, 2016.

\bibitem[Huang et~al.(2017)Huang, Liu, van~der Maaten, and
  Weinberger]{densenet}
Gao Huang, Zhuang Liu, Laurens van~der Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Huang et~al.(2018)Huang, Liu, Van~der Maaten, and
  Weinberger]{huang2018condensenet}
Gao Huang, Shichen Liu, Laurens Van~der Maaten, and Kilian~Q Weinberger.
\newblock Condensenet: An efficient densenet using learned group convolutions.
\newblock In \emph{CVPR}, 2018.

\bibitem[Huang \& Wang(2018)Huang and Wang]{huang2018data}
Zehao Huang and Naiyan Wang.
\newblock Data-driven sparse structure selection for deep neural networks.
\newblock \emph{ECCV}, 2018.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{bn}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Jia et~al.(2014)Jia, Shelhamer, Donahue, Karayev, Long, Girshick,
  Guadarrama, and Darrell]{caffe}
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross
  Girshick, Sergio Guadarrama, and Trevor Darrell.
\newblock Caffe: Convolutional architecture for fast feature embedding.
\newblock In \emph{ACM Multimedia}, 2014.

\bibitem[Krizhevsky(2009)]{cifar}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Lebedev \& Lempitsky(2016)Lebedev and Lempitsky]{gs2}
Vadim Lebedev and Victor Lempitsky.
\newblock Fast convnets using group-wise brain damage.
\newblock In \emph{CVPR}, 2016.

\bibitem[Lebedev et~al.(2014)Lebedev, Ganin, Rakhuba, Oseledets, and
  Lempitsky]{lowrank2}
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor
  Lempitsky.
\newblock Speeding-up convolutional neural networks using fine-tuned
  cp-decomposition.
\newblock \emph{ICLR}, 2014.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{obd}
Yann LeCun, John~S Denker, and Sara~A Solla.
\newblock Optimal brain damage.
\newblock In \emph{NIPS}, 1990.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, Haffner,
  et~al.]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, Patrick Haffner, et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 1998.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock In \emph{ICLR}, 2017.

\bibitem[Lin et~al.(2017)Lin, Rao, Lu, and Zhou]{lin2017runtime}
Ji~Lin, Yongming Rao, Jiwen Lu, and Jie Zhou.
\newblock Runtime neural pruning.
\newblock In \emph{NIPS}, 2017.

\bibitem[Liu et~al.(2018{\natexlab{a}})Liu, Simonyan, Vinyals, Fernando, and
  Kavukcuoglu]{liu2017hierarchical}
Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray
  Kavukcuoglu.
\newblock Hierarchical representations for efficient architecture search.
\newblock \emph{ICLR}, 2018{\natexlab{a}}.

\bibitem[Liu et~al.(2018{\natexlab{b}})Liu, Simonyan, and Yang]{darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Darts: Differentiable architecture search.
\newblock \emph{arXiv preprint arXiv:1806.09055}, 2018{\natexlab{b}}.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{liu2017learning}
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui
  Zhang.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{ICCV}, 2017.

\bibitem[Long et~al.(2015)Long, Shelhamer, and Darrell]{fcn}
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In \emph{CVPR}, 2015.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{l0sparse}
Christos Louizos, Max Welling, and Diederik~P Kingma.
\newblock Learning sparse neural networks through $ l\_0 $ regularization.
\newblock \emph{ICLR}, 2018.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{luo2017thinet}
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In \emph{ICCV}, 2017.

\bibitem[Mittal et~al.(2018)Mittal, Bhardwaj, Khapra, and
  Ravindran]{recovering}
Deepak Mittal, Shweta Bhardwaj, Mitesh~M Khapra, and Balaraman Ravindran.
\newblock Recovering from random pruning: On the plasticity of deep
  convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1801.10447}, 2018.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and Vetrov]{vdropout}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{nvidia}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{arXiv preprint arXiv:1611.06440}, 2016.

\bibitem[P.~Kingma et~al.(2015)P.~Kingma, Salimans, and Welling]{vdropoutorg}
Diederik P.~Kingma, Tim Salimans, and Max Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{NIPS}, 2015.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{pytorch}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS Workshop}, 2017.

\bibitem[Pham et~al.(2018)Pham, Guan, Zoph, Le, and Dean]{sharing}
Hieu Pham, Melody~Y Guan, Barret Zoph, Quoc~V Le, and Jeff Dean.
\newblock Efficient neural architecture search via parameter sharing.
\newblock \emph{ICML}, 2018.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{xnornet}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{ECCV}, 2016.

\bibitem[Ren et~al.(2015)Ren, He, Girshick, and Sun]{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In \emph{NIPS}, 2015.

\bibitem[Romero et~al.(2015)Romero, Ballas, Kahou, Chassang, Gatta, and
  Bengio]{fitnet}
Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo
  Gatta, and Yoshua Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock \emph{ICLR}, 2015.

\bibitem[Shen et~al.(2017)Shen, Liu, Li, Jiang, Chen, and Xue]{dsod}
Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang, Yurong Chen, and
  Xiangyang Xue.
\newblock Dsod: Learning deeply supervised object detectors from scratch.
\newblock In \emph{ICCV}, 2017.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{ICLR}, 2015.

\bibitem[Srinivas \& Babu(2015)Srinivas and Babu]{datafree}
Suraj Srinivas and R~Venkatesh Babu.
\newblock Data-free parameter pruning for deep neural networks.
\newblock \emph{BMVC}, 2015.

\bibitem[Suau et~al.(2018)Suau, Zappella, Palakkode, and Apostoloff]{pfa}
Xavier Suau, Luca Zappella, Vinay Palakkode, and Nicholas Apostoloff.
\newblock Principal filter analysis for guided network compression.
\newblock \emph{arXiv preprint arXiv:1807.10585}, 2018.

\bibitem[Wang et~al.(2017)Wang, Yu, Dou, and Gonzalez]{wang2017skipnet}
Xin Wang, Fisher Yu, Zi-Yi Dou, and Joseph~E Gonzalez.
\newblock Skipnet: Learning dynamic routing in convolutional networks.
\newblock \emph{arXiv preprint arXiv:1711.09485}, 2017.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{NIPS}, 2016.

\bibitem[Xie \& Yuille(2017)Xie and Yuille]{genetic}
Lingxi Xie and Alan~L Yuille.
\newblock Genetic cnn.
\newblock In \emph{ICCV}, 2017.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Yang et~al.(2017)Yang, Lu, Batra, and Parikh]{jjfaster2rcnn}
Jianwei Yang, Jiasen Lu, Dhruv Batra, and Devi Parikh.
\newblock A faster pytorch implementation of faster r-cnn.
\newblock \emph{https://github.com/jwyang/faster-rcnn.pytorch}, 2017.

\bibitem[Ye et~al.(2018)Ye, Lu, Lin, and Wang]{ye2018rethinking}
Jianbo Ye, Xin Lu, Zhe Lin, and James~Z Wang.
\newblock Rethinking the smaller-norm-less-informative assumption in channel
  pruning of convolution layers.
\newblock \emph{ICLR}, 2018.

\bibitem[Yu et~al.(2018)Yu, Li, Chen, Lai, Morariu, Han, Gao, Lin, and
  Davis]{nisp}
Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad~I Morariu, Xintong Han,
  Mingfei Gao, Ching-Yung Lin, and Larry~S Davis.
\newblock Nisp: Pruning networks using neuron importance score propagation.
\newblock In \emph{CVPR}, 2018.

\bibitem[Zhou et~al.(2016)Zhou, Alvarez, and Porikli]{gs3}
Hao Zhou, Jose~M Alvarez, and Fatih Porikli.
\newblock Less is more: Towards compact cnns.
\newblock In \emph{ECCV}, 2016.

\bibitem[Zhu \& Gupta(2018)Zhu and Gupta]{toprune}
Michael Zhu and Suyog Gupta.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock \emph{ICLR Workshop}, 2018.

\bibitem[Zoph \& Le(2017)Zoph and Le]{rl1}
Barret Zoph and Quoc~V Le.
\newblock Neural architecture search with reinforcement learning.
\newblock \emph{ICLR}, 2017.

\end{thebibliography}
