\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Rethinking the Value of Network Pruning}{2}{chapter.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Overview}{2}{section.2.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A typical three-stage network pruning pipeline.\relax }}{3}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig1}{{\M@TitleReference {2.1}{A typical three-stage network pruning pipeline.\relax }}{3}{A typical three-stage network pruning pipeline.\relax }{figure.caption.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Introduction}{3}{section.2.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Difference between predefined and automatically discovered target architectures, in channel pruning as an example. The pruning ratio $x$ is user-specified, while $a, b, c, d$ are determined by the pruning algorithm. Unstructured sparse pruning can also be viewed as automatic.\relax }}{4}{figure.caption.5}\protected@file@percent }
\newlabel{auto}{{\M@TitleReference {2.2}{Difference between predefined and automatically discovered target architectures, in channel pruning as an example. The pruning ratio $x$ is user-specified, while $a, b, c, d$ are determined by the pruning algorithm. Unstructured sparse pruning can also be viewed as automatic.\relax }}{4}{Difference between predefined and automatically discovered target architectures, in channel pruning as an example. The pruning ratio $x$ is user-specified, while $a, b, c, d$ are determined by the pruning algorithm. Unstructured sparse pruning can also be viewed as automatic.\relax }{figure.caption.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Background}{5}{section.2.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}Methodology}{6}{section.2.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Target Pruned Architectures}{6}{section*.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Datasets, Network Architectures and Pruning Methods}{7}{section*.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Training Budget}{7}{section*.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Implementation}{8}{section*.9}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.5}Experiments}{8}{section.2.5}\protected@file@percent }
\newlabel{sec:exp}{{\M@TitleReference {2.5}{Experiments}}{8}{Experiments}{section.2.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Predefined Structured Pruning}{8}{section*.10}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Results (accuracy) for $L_1$-norm based filter pruning~\cite {li2016pruning}. ``Pruned Model'' is the model pruned from the large model. Configurations of Model and Pruned Model are both from the original paper. \relax }}{9}{table.caption.11}\protected@file@percent }
\newlabel{pruning-filters}{{\M@TitleReference {2.1}{Results (accuracy) for $L_1$-norm based filter pruning~\cite {li2016pruning}. ``Pruned Model'' is the model pruned from the large model. Configurations of Model and Pruned Model are both from the original paper. \relax }}{9}{Results (accuracy) for $L_1$-norm based filter pruning~\cite {li2016pruning}. ``Pruned Model'' is the model pruned from the large model. Configurations of Model and Pruned Model are both from the original paper. \relax }{table.caption.11}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Results (accuracy) for ThiNet~\cite {luo2017thinet}. Names such as ``VGG-GAP'' and ``ResNet50-30\%'' are pruned models whose configurations are defined in~\cite {luo2017thinet}. To accommodate the effects of different frameworks between our implementation and the original paper's, we compare relative accuracy drop from the unpruned large model. For example, for the pruned model VGG-Conv, $-$1.23 is relative to 71.03 on the left, which is the reported accuracy of the unpruned large model VGG-16 in the original paper; $-$2.75 is relative to 71.51 on the left, which is VGG-16's accuracy in our implementation. \relax }}{9}{table.caption.12}\protected@file@percent }
\newlabel{thinet}{{\M@TitleReference {2.2}{Results (accuracy) for ThiNet~\cite {luo2017thinet}. Names such as ``VGG-GAP'' and ``ResNet50-30\%'' are pruned models whose configurations are defined in~\cite {luo2017thinet}. To accommodate the effects of different frameworks between our implementation and the original paper's, we compare relative accuracy drop from the unpruned large model. For example, for the pruned model VGG-Conv, $-$1.23 is relative to 71.03 on the left, which is the reported accuracy of the unpruned large model VGG-16 in the original paper; $-$2.75 is relative to 71.51 on the left, which is VGG-16's accuracy in our implementation. \relax }}{9}{Results (accuracy) for ThiNet~\cite {luo2017thinet}. Names such as ``VGG-GAP'' and ``ResNet50-30\%'' are pruned models whose configurations are defined in~\cite {luo2017thinet}. To accommodate the effects of different frameworks between our implementation and the original paper's, we compare relative accuracy drop from the unpruned large model. For example, for the pruned model VGG-Conv, $-$1.23 is relative to 71.03 on the left, which is the reported accuracy of the unpruned large model VGG-16 in the original paper; $-$2.75 is relative to 71.51 on the left, which is VGG-16's accuracy in our implementation. \relax }{table.caption.12}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Results (accuracy) for Regression based Feature Reconstruction~\cite {he2017channel}. Pruned models such as ``VGG-16-5x'' are defined in~\cite {he2017channel}. Similar to \autoref  {thinet}, we compare relative accuracy drop from unpruned large models.\relax }}{10}{table.caption.13}\protected@file@percent }
\newlabel{channel-pruning}{{\M@TitleReference {2.3}{Results (accuracy) for Regression based Feature Reconstruction~\cite {he2017channel}. Pruned models such as ``VGG-16-5x'' are defined in~\cite {he2017channel}. Similar to \autoref  {thinet}, we compare relative accuracy drop from unpruned large models.\relax }}{10}{Results (accuracy) for Regression based Feature Reconstruction~\cite {he2017channel}. Pruned models such as ``VGG-16-5x'' are defined in~\cite {he2017channel}. Similar to \autoref {thinet}, we compare relative accuracy drop from unpruned large models.\relax }{table.caption.13}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Automatic Structured Pruning}{10}{section*.14}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Results (accuracy) for Network Slimming~\cite {liu2017learning}. ``Prune ratio'' stands for total percentage of channels that are pruned in the whole network. The same ratios for each model are used as the original paper. \relax }}{11}{table.caption.15}\protected@file@percent }
\newlabel{network-slimming}{{\M@TitleReference {2.4}{Results (accuracy) for Network Slimming~\cite {liu2017learning}. ``Prune ratio'' stands for total percentage of channels that are pruned in the whole network. The same ratios for each model are used as the original paper. \relax }}{11}{Results (accuracy) for Network Slimming~\cite {liu2017learning}. ``Prune ratio'' stands for total percentage of channels that are pruned in the whole network. The same ratios for each model are used as the original paper. \relax }{table.caption.15}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces Results (accuracy) for residual block pruning using Sparse Structure Selection~\cite {huang2018data}. In the original paper no fine-tuning is required so there is a ``Pruned'' column instead of ``Fine-tuned'' as before. \relax }}{11}{table.caption.16}\protected@file@percent }
\newlabel{sparse}{{\M@TitleReference {2.5}{Results (accuracy) for residual block pruning using Sparse Structure Selection~\cite {huang2018data}. In the original paper no fine-tuning is required so there is a ``Pruned'' column instead of ``Fine-tuned'' as before. \relax }}{11}{Results (accuracy) for residual block pruning using Sparse Structure Selection~\cite {huang2018data}. In the original paper no fine-tuning is required so there is a ``Pruned'' column instead of ``Fine-tuned'' as before. \relax }{table.caption.16}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Unstructured Magnitude-based Pruning}{11}{section*.17}\protected@file@percent }
\newlabel{sec:unstructured}{{\M@TitleReference {2.5}{Unstructured Magnitude-based Pruning}}{11}{Unstructured Magnitude-based Pruning}{section*.17}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces Results (accuracy) for unstructured pruning~\cite {han2015learning}. ``Prune Ratio'' denotes the percentage of parameters pruned in the set of all convolutional weights. \relax }}{12}{table.caption.18}\protected@file@percent }
\newlabel{weight-level}{{\M@TitleReference {2.6}{Results (accuracy) for unstructured pruning~\cite {han2015learning}. ``Prune Ratio'' denotes the percentage of parameters pruned in the set of all convolutional weights. \relax }}{12}{Results (accuracy) for unstructured pruning~\cite {han2015learning}. ``Prune Ratio'' denotes the percentage of parameters pruned in the set of all convolutional weights. \relax }{table.caption.18}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.6}Network Pruning as Architecture search}{12}{section.2.6}\protected@file@percent }
\newlabel{sec:arch}{{\M@TitleReference {2.6}{Network Pruning as Architecture search}}{12}{Network Pruning as Architecture search}{section.2.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Parameter Efficiency of Pruned Architectures}{13}{section*.19}\protected@file@percent }
\newlabel{arch-search-a1}{{\M@TitleReference {\caption@xref {arch-search-a1}{ on input line 308}}{Parameter Efficiency of Pruned Architectures}}{13}{Parameter Efficiency of Pruned Architectures}{figure.caption.20}{}}
\newlabel{sub@arch-search-a1}{{\M@TitleReference {}{Parameter Efficiency of Pruned Architectures}}{13}{Parameter Efficiency of Pruned Architectures}{figure.caption.20}{}}
\newlabel{arch-search-b1}{{\M@TitleReference {\caption@xref {arch-search-b1}{ on input line 315}}{Parameter Efficiency of Pruned Architectures}}{13}{Parameter Efficiency of Pruned Architectures}{figure.caption.20}{}}
\newlabel{sub@arch-search-b1}{{\M@TitleReference {}{Parameter Efficiency of Pruned Architectures}}{13}{Parameter Efficiency of Pruned Architectures}{figure.caption.20}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  Pruned architectures obtained by different approaches, all \emph  {trained from scratch}, averaged over 5 runs. Architectures obtained by automatic pruning methods (\emph  {Left:} Network Slimming~\cite {liu2017learning}, \emph  {Right:} Unstructured pruning~\cite {han2015learning}) have better parameter efficiency than uniformly pruning channels or sparsifying weights in the whole network. \relax }}{13}{figure.caption.20}\protected@file@percent }
\newlabel{arch-search}{{\M@TitleReference {2.3}{ Pruned architectures obtained by different approaches, all \emph  {trained from scratch}, averaged over 5 runs. Architectures obtained by automatic pruning methods (\emph  {Left:} Network Slimming~\cite {liu2017learning}, \emph  {Right:} Unstructured pruning~\cite {han2015learning}) have better parameter efficiency than uniformly pruning channels or sparsifying weights in the whole network. \relax }}{13}{ Pruned architectures obtained by different approaches, all \emph {trained from scratch}, averaged over 5 runs. Architectures obtained by automatic pruning methods (\emph {Left:} Network Slimming~\cite {liu2017learning}, \emph {Right:} Unstructured pruning~\cite {han2015learning}) have better parameter efficiency than uniformly pruning channels or sparsifying weights in the whole network. \relax }{figure.caption.20}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.7}{\ignorespaces Network architectures obtained by pruning 60\% channels on VGG-16 (in total 13 conv-layers) using Network Slimming. Width and Width* are number of channels in the original and pruned architectures, averaged over 5 runs.\relax }}{14}{table.caption.21}\protected@file@percent }
\newlabel{width}{{\M@TitleReference {2.7}{Network architectures obtained by pruning 60\% channels on VGG-16 (in total 13 conv-layers) using Network Slimming. Width and Width* are number of channels in the original and pruned architectures, averaged over 5 runs.\relax }}{14}{Network architectures obtained by pruning 60\% channels on VGG-16 (in total 13 conv-layers) using Network Slimming. Width and Width* are number of channels in the original and pruned architectures, averaged over 5 runs.\relax }{table.caption.21}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The average sparsity pattern of all 3$\times $3 convolutional kernels in certain layer stages in a unstructured pruned VGG-16. Darker color means higher probability of weight being kept.\relax }}{14}{table.caption.21}\protected@file@percent }
\newlabel{sparsity}{{\M@TitleReference {2.4}{The average sparsity pattern of all 3$\times $3 convolutional kernels in certain layer stages in a unstructured pruned VGG-16. Darker color means higher probability of weight being kept.\relax }}{14}{The average sparsity pattern of all 3$\times $3 convolutional kernels in certain layer stages in a unstructured pruned VGG-16. Darker color means higher probability of weight being kept.\relax }{table.caption.21}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  Pruned architectures obtained by different approaches, \emph  {all trained from scratch}, averaged over 5 runs. \emph  {Left:} Results for PreResNet-164 pruned on CIFAR-10 by Network Slimming~\cite {liu2017learning}. \emph  {Middle} and \emph  {Right}: Results for PreResNet-110 and DenseNet-40 pruned on CIFAR-100 by unstructured pruning~\cite {han2015learning}. \relax }}{14}{figure.caption.22}\protected@file@percent }
\newlabel{arch-search-negative}{{\M@TitleReference {2.5}{ Pruned architectures obtained by different approaches, \emph  {all trained from scratch}, averaged over 5 runs. \emph  {Left:} Results for PreResNet-164 pruned on CIFAR-10 by Network Slimming~\cite {liu2017learning}. \emph  {Middle} and \emph  {Right}: Results for PreResNet-110 and DenseNet-40 pruned on CIFAR-100 by unstructured pruning~\cite {han2015learning}. \relax }}{14}{ Pruned architectures obtained by different approaches, \emph {all trained from scratch}, averaged over 5 runs. \emph {Left:} Results for PreResNet-164 pruned on CIFAR-10 by Network Slimming~\cite {liu2017learning}. \emph {Middle} and \emph {Right}: Results for PreResNet-110 and DenseNet-40 pruned on CIFAR-100 by unstructured pruning~\cite {han2015learning}. \relax }{figure.caption.22}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{More Analysis}{14}{section*.23}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Generalizable Design Principles from Pruned Architectures}{15}{section*.24}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Pruned architectures obtained by different approaches, \emph  {all trained from scratch}, averaged over 5 runs. ``Guided Pruning/Sparsification'' means using the average sparsity patterns in each layer stage to design the network; ``Transferred Guided Pruning/Sparsification'' means using the sparsity patterns obtained by a pruned VGG-16 on CIFAR-10, to design the network for VGG-19 on CIFAR-100. Following the design guidelines provided by the pruned architectures, we achieve better parameter efficiency, even when the guidelines are transferred from another dataset and model.\relax }}{15}{figure.caption.25}\protected@file@percent }
\newlabel{arch-search-2}{{\M@TitleReference {2.6}{ Pruned architectures obtained by different approaches, \emph  {all trained from scratch}, averaged over 5 runs. ``Guided Pruning/Sparsification'' means using the average sparsity patterns in each layer stage to design the network; ``Transferred Guided Pruning/Sparsification'' means using the sparsity patterns obtained by a pruned VGG-16 on CIFAR-10, to design the network for VGG-19 on CIFAR-100. Following the design guidelines provided by the pruned architectures, we achieve better parameter efficiency, even when the guidelines are transferred from another dataset and model.\relax }}{15}{ Pruned architectures obtained by different approaches, \emph {all trained from scratch}, averaged over 5 runs. ``Guided Pruning/Sparsification'' means using the average sparsity patterns in each layer stage to design the network; ``Transferred Guided Pruning/Sparsification'' means using the sparsity patterns obtained by a pruned VGG-16 on CIFAR-10, to design the network for VGG-19 on CIFAR-100. Following the design guidelines provided by the pruned architectures, we achieve better parameter efficiency, even when the guidelines are transferred from another dataset and model.\relax }{figure.caption.25}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Discussions with Conventional Architecture Search Methods}{16}{section*.26}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.7}Experimenting with Lottery Ticket Hypothesis}{16}{section.2.7}\protected@file@percent }
\newlabel{ap:lottery}{{\M@TitleReference {2.7}{Experimenting with Lottery Ticket Hypothesis}}{16}{Experimenting with Lottery Ticket Hypothesis}{section.2.7}{}}
\newlabel{iterative-1}{{\M@TitleReference {2.7a}{Iterative Pruning\relax }}{17}{Iterative Pruning\relax }{figure.caption.27}{}}
\newlabel{sub@iterative-1}{{\M@TitleReference {a}{Iterative Pruning\relax }}{17}{Iterative Pruning\relax }{figure.caption.27}{}}
\newlabel{iterative-3}{{\M@TitleReference {2.7b}{One-shot Pruning\relax }}{17}{One-shot Pruning\relax }{figure.caption.27}{}}
\newlabel{sub@iterative-3}{{\M@TitleReference {b}{One-shot Pruning\relax }}{17}{One-shot Pruning\relax }{figure.caption.27}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces  Comparisons with the Lottery Ticket Hypothesis~\cite {lottery} for iterative/one-shot unstructured pruning~\cite {han2015learning} with two initial learning rates 0.1 and 0.01, on CIFAR-10 dataset. Each point is averaged over 5 runs. Using the winning ticket as initialization only brings improvement when the learning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).\relax }}{17}{figure.caption.27}\protected@file@percent }
\newlabel{lottery-figure-1}{{\M@TitleReference {2.7}{ Comparisons with the Lottery Ticket Hypothesis~\cite {lottery} for iterative/one-shot unstructured pruning~\cite {han2015learning} with two initial learning rates 0.1 and 0.01, on CIFAR-10 dataset. Each point is averaged over 5 runs. Using the winning ticket as initialization only brings improvement when the learning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).\relax }}{17}{ Comparisons with the Lottery Ticket Hypothesis~\cite {lottery} for iterative/one-shot unstructured pruning~\cite {han2015learning} with two initial learning rates 0.1 and 0.01, on CIFAR-10 dataset. Each point is averaged over 5 runs. Using the winning ticket as initialization only brings improvement when the learning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).\relax }{figure.caption.27}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.8}{\ignorespaces Comparisons with the Lottery Ticket Hypothesis~\cite {lottery} on a structured pruning method ($L_1$-norm based filter pruning~\cite {li2016pruning}) with two initial learning rates: 0.1 and 0.01. In both cases, using winning tickets does not bring improvement on accuracy.\relax }}{18}{table.caption.28}\protected@file@percent }
\newlabel{lottery-2}{{\M@TitleReference {2.8}{Comparisons with the Lottery Ticket Hypothesis~\cite {lottery} on a structured pruning method ($L_1$-norm based filter pruning~\cite {li2016pruning}) with two initial learning rates: 0.1 and 0.01. In both cases, using winning tickets does not bring improvement on accuracy.\relax }}{18}{Comparisons with the Lottery Ticket Hypothesis~\cite {lottery} on a structured pruning method ($L_1$-norm based filter pruning~\cite {li2016pruning}) with two initial learning rates: 0.1 and 0.01. In both cases, using winning tickets does not bring improvement on accuracy.\relax }{table.caption.28}{}}
\newlabel{subtable-3}{{\M@TitleReference {2.9a}{One-shot pruning with initial learning rate 0.1\relax }}{19}{One-shot pruning with initial learning rate 0.1\relax }{table.caption.29}{}}
\newlabel{sub@subtable-3}{{\M@TitleReference {a}{One-shot pruning with initial learning rate 0.1\relax }}{19}{One-shot pruning with initial learning rate 0.1\relax }{table.caption.29}{}}
\newlabel{subtable-4}{{\M@TitleReference {2.9b}{One-shot pruning with initial learning rate 0.01\relax }}{19}{One-shot pruning with initial learning rate 0.01\relax }{table.caption.29}{}}
\newlabel{sub@subtable-4}{{\M@TitleReference {b}{One-shot pruning with initial learning rate 0.01\relax }}{19}{One-shot pruning with initial learning rate 0.01\relax }{table.caption.29}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.9}{\ignorespaces Comparisons with the Lottery Ticket Hypothesis~\cite {lottery} for one-shot unstructured pruning~\cite {han2015learning} with two initial learning rates: 0.1 and 0.01. The same results are visualized in \autoref  {iterative-3}. Using the winning ticket as initialization only brings improvement when the learning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).\relax }}{19}{table.caption.29}\protected@file@percent }
\newlabel{lottery-1}{{\M@TitleReference {2.9}{Comparisons with the Lottery Ticket Hypothesis~\cite {lottery} for one-shot unstructured pruning~\cite {han2015learning} with two initial learning rates: 0.1 and 0.01. The same results are visualized in \autoref  {iterative-3}. Using the winning ticket as initialization only brings improvement when the learning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).\relax }}{19}{Comparisons with the Lottery Ticket Hypothesis~\cite {lottery} for one-shot unstructured pruning~\cite {han2015learning} with two initial learning rates: 0.1 and 0.01. The same results are visualized in \autoref {iterative-3}. Using the winning ticket as initialization only brings improvement when the learning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).\relax }{table.caption.29}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.8}Additional Studies}{20}{section.2.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Results on Soft Filter Pruning}{20}{section*.30}\protected@file@percent }
\newlabel{sec:sfp}{{\M@TitleReference {2.8}{Results on Soft Filter Pruning}}{20}{Results on Soft Filter Pruning}{section*.30}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.10}{\ignorespaces Results (accuracy) for Soft Filter Pruning~\cite {he2018sfp} without pretrained models.\relax }}{20}{table.caption.31}\protected@file@percent }
\newlabel{sfp-1}{{\M@TitleReference {2.10}{Results (accuracy) for Soft Filter Pruning~\cite {he2018sfp} without pretrained models.\relax }}{20}{Results (accuracy) for Soft Filter Pruning~\cite {he2018sfp} without pretrained models.\relax }{table.caption.31}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.11}{\ignorespaces Results (accuracy) for Soft Filter Pruning~\cite {he2018sfp} using pretrained models.\relax }}{21}{table.caption.32}\protected@file@percent }
\newlabel{sfp-2}{{\M@TitleReference {2.11}{Results (accuracy) for Soft Filter Pruning~\cite {he2018sfp} using pretrained models.\relax }}{21}{Results (accuracy) for Soft Filter Pruning~\cite {he2018sfp} using pretrained models.\relax }{table.caption.32}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Transfer Learning to Object Detection}{21}{section*.33}\protected@file@percent }
\newlabel{sec:transfer}{{\M@TitleReference {2.8}{Transfer Learning to Object Detection}}{21}{Transfer Learning to Object Detection}{section*.33}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.12}{\ignorespaces Results (mAP) for pruning on detection task. The pruned models are chosen from~\cite {li2016pruning}. Prune-C refers to pruning on classifcation pre-trained weights, Prune-D refers to pruning after the weights are transferred to detection task. Scratch-E/B means pre-training the pruned model from scratch on classification and transfer to detection.\relax }}{21}{table.caption.34}\protected@file@percent }
\newlabel{det-prune}{{\M@TitleReference {2.12}{Results (mAP) for pruning on detection task. The pruned models are chosen from~\cite {li2016pruning}. Prune-C refers to pruning on classifcation pre-trained weights, Prune-D refers to pruning after the weights are transferred to detection task. Scratch-E/B means pre-training the pruned model from scratch on classification and transfer to detection.\relax }}{21}{Results (mAP) for pruning on detection task. The pruned models are chosen from~\cite {li2016pruning}. Prune-C refers to pruning on classifcation pre-trained weights, Prune-D refers to pruning after the weights are transferred to detection task. Scratch-E/B means pre-training the pruned model from scratch on classification and transfer to detection.\relax }{table.caption.34}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Aggressively Pruned Models}{22}{section*.35}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.13}{\ignorespaces Results (accuracy) for Network Slimming~\cite {liu2017learning} when the models are aggressively pruned. ``Prune ratio'' stands for total percentage of channels that are pruned in the whole network. Larger ratios are used than the original paper of~\cite {liu2017learning}. \relax }}{22}{table.caption.36}\protected@file@percent }
\newlabel{significant}{{\M@TitleReference {2.13}{Results (accuracy) for Network Slimming~\cite {liu2017learning} when the models are aggressively pruned. ``Prune ratio'' stands for total percentage of channels that are pruned in the whole network. Larger ratios are used than the original paper of~\cite {liu2017learning}. \relax }}{22}{Results (accuracy) for Network Slimming~\cite {liu2017learning} when the models are aggressively pruned. ``Prune ratio'' stands for total percentage of channels that are pruned in the whole network. Larger ratios are used than the original paper of~\cite {liu2017learning}. \relax }{table.caption.36}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.14}{\ignorespaces Results (accuracy) for $L_1$-norm based filter pruning~\cite {li2016pruning} when the models are aggressively pruned. \relax }}{22}{table.caption.37}\protected@file@percent }
\newlabel{significant-2}{{\M@TitleReference {2.14}{Results (accuracy) for $L_1$-norm based filter pruning~\cite {li2016pruning} when the models are aggressively pruned. \relax }}{22}{Results (accuracy) for $L_1$-norm based filter pruning~\cite {li2016pruning} when the models are aggressively pruned. \relax }{table.caption.37}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Extending Fine-tuning Epochs}{22}{section*.39}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.15}{\ignorespaces Results (accuracy) for unstructured pruning~\cite {han2015learning} when the models are aggressively pruned. \relax }}{23}{table.caption.38}\protected@file@percent }
\newlabel{significant-3}{{\M@TitleReference {2.15}{Results (accuracy) for unstructured pruning~\cite {han2015learning} when the models are aggressively pruned. \relax }}{23}{Results (accuracy) for unstructured pruning~\cite {han2015learning} when the models are aggressively pruned. \relax }{table.caption.38}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.16}{\ignorespaces ``Fine-tune-40'' stands for fine-tuning 40 epochs and so on. Scratch-E models are trained for 160 epochs. We observe that fine-tuning for more epochs does not help improve the accuracy much, and models trained from scratch can still perform on par with fine-tuned models. \relax }}{23}{table.caption.40}\protected@file@percent }
\newlabel{finetune-more}{{\M@TitleReference {2.16}{``Fine-tune-40'' stands for fine-tuning 40 epochs and so on. Scratch-E models are trained for 160 epochs. We observe that fine-tuning for more epochs does not help improve the accuracy much, and models trained from scratch can still perform on par with fine-tuned models. \relax }}{23}{``Fine-tune-40'' stands for fine-tuning 40 epochs and so on. Scratch-E models are trained for 160 epochs. We observe that fine-tuning for more epochs does not help improve the accuracy much, and models trained from scratch can still perform on par with fine-tuned models. \relax }{table.caption.40}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Extending the Standard Training Schedule}{23}{section*.41}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.17}{\ignorespaces Results for $L_1$-norm filter pruning~\cite {li2016pruning} when the training schedule of the large model is extended from 160 to 300 epochs. \relax }}{24}{table.caption.42}\protected@file@percent }
\newlabel{enough-epochs}{{\M@TitleReference {2.17}{Results for $L_1$-norm filter pruning~\cite {li2016pruning} when the training schedule of the large model is extended from 160 to 300 epochs. \relax }}{24}{Results for $L_1$-norm filter pruning~\cite {li2016pruning} when the training schedule of the large model is extended from 160 to 300 epochs. \relax }{table.caption.42}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Weight Distributions}{24}{section*.43}\protected@file@percent }
\newlabel{sec:dist}{{\M@TitleReference {2.8}{Weight Distributions}}{24}{Weight Distributions}{section*.43}{}}
\newlabel{arch-search-b1}{{\M@TitleReference {\caption@xref {arch-search-b1}{ on input line 789}}{Weight Distributions}}{24}{Weight Distributions}{figure.caption.44}{}}
\newlabel{sub@arch-search-b1}{{\M@TitleReference {}{Weight Distributions}}{24}{Weight Distributions}{figure.caption.44}{}}
\newlabel{arch-search-b}{{\M@TitleReference {\caption@xref {arch-search-b}{ on input line 798}}{Weight Distributions}}{24}{Weight Distributions}{figure.caption.44}{}}
\newlabel{sub@arch-search-b}{{\M@TitleReference {}{Weight Distributions}}{24}{Weight Distributions}{figure.caption.44}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Weight distribution of convolutional layers for different pruning methods. We use VGG-16 and CIFAR-10 for this visualization. We compare the weight distribution of unpruned models, fine-tuned models and scratch-trained models. \emph  {Top}: Results for Network Slimming~\cite {liu2017learning}. \emph  {Bottom}: Results for unstructured pruning~\cite {han2015learning}.\relax }}{24}{figure.caption.44}\protected@file@percent }
\newlabel{dist}{{\M@TitleReference {2.8}{Weight distribution of convolutional layers for different pruning methods. We use VGG-16 and CIFAR-10 for this visualization. We compare the weight distribution of unpruned models, fine-tuned models and scratch-trained models. \emph  {Top}: Results for Network Slimming~\cite {liu2017learning}. \emph  {Bottom}: Results for unstructured pruning~\cite {han2015learning}.\relax }}{24}{Weight distribution of convolutional layers for different pruning methods. We use VGG-16 and CIFAR-10 for this visualization. We compare the weight distribution of unpruned models, fine-tuned models and scratch-trained models. \emph {Top}: Results for Network Slimming~\cite {liu2017learning}. \emph {Bottom}: Results for unstructured pruning~\cite {han2015learning}.\relax }{figure.caption.44}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{More Sparsity Patterns for Pruned Architectures}{25}{section*.45}\protected@file@percent }
\newlabel{sec:additional}{{\M@TitleReference {2.8}{More Sparsity Patterns for Pruned Architectures}}{25}{More Sparsity Patterns for Pruned Architectures}{section*.45}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.18}{\ignorespaces  Sparsity patterns of PreResNet-164 pruned on CIFAR-10 by Network Slimming shown in \autoref  {arch-search-negative} (left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channels to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).\relax }}{25}{table.caption.46}\protected@file@percent }
\newlabel{sparsity-5}{{\M@TitleReference {2.18}{ Sparsity patterns of PreResNet-164 pruned on CIFAR-10 by Network Slimming shown in \autoref  {arch-search-negative} (left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channels to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).\relax }}{25}{ Sparsity patterns of PreResNet-164 pruned on CIFAR-10 by Network Slimming shown in \autoref {arch-search-negative} (left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channels to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).\relax }{table.caption.46}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.19}{\ignorespaces  Average sparsity patterns of 3$\times $3 kernels of PreResNet-110 pruned on CIFAR-100 by unstructured pruning shown in \autoref  {arch-search-negative} (middle) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).\relax }}{26}{table.caption.47}\protected@file@percent }
\newlabel{sparsity-6}{{\M@TitleReference {2.19}{ Average sparsity patterns of 3$\times $3 kernels of PreResNet-110 pruned on CIFAR-100 by unstructured pruning shown in \autoref  {arch-search-negative} (middle) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).\relax }}{26}{ Average sparsity patterns of 3$\times $3 kernels of PreResNet-110 pruned on CIFAR-100 by unstructured pruning shown in \autoref {arch-search-negative} (middle) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).\relax }{table.caption.47}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.20}{\ignorespaces  Average sparsity patterns of 3$\times $3 kernels of DenseNet-40 pruned on CIFAR-100 by unstructured pruning shown in \autoref  {arch-search-negative} (right) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).\relax }}{26}{table.caption.48}\protected@file@percent }
\newlabel{sparsity-8}{{\M@TitleReference {2.20}{ Average sparsity patterns of 3$\times $3 kernels of DenseNet-40 pruned on CIFAR-100 by unstructured pruning shown in \autoref  {arch-search-negative} (right) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).\relax }}{26}{ Average sparsity patterns of 3$\times $3 kernels of DenseNet-40 pruned on CIFAR-100 by unstructured pruning shown in \autoref {arch-search-negative} (right) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform (across stages).\relax }{table.caption.48}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.21}{\ignorespaces  Sparsity patterns of VGG-16 pruned on CIFAR-10 by Network Slimming shown in Figure~\ref  {arch-search} (left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channels to be kept. For each prune ratio, the latter stages tend to have more redundancy than earlier stages.\relax }}{27}{table.caption.49}\protected@file@percent }
\newlabel{sparsity-2}{{\M@TitleReference {2.21}{ Sparsity patterns of VGG-16 pruned on CIFAR-10 by Network Slimming shown in Figure~\ref  {arch-search} (left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channels to be kept. For each prune ratio, the latter stages tend to have more redundancy than earlier stages.\relax }}{27}{ Sparsity patterns of VGG-16 pruned on CIFAR-10 by Network Slimming shown in Figure~\ref {arch-search} (left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channels to be kept. For each prune ratio, the latter stages tend to have more redundancy than earlier stages.\relax }{table.caption.49}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.9}Discussion and Conclusion}{27}{section.2.9}\protected@file@percent }
\@setckpt{chap3}{
\setcounter{page}{28}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{-1}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{9}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{1}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{39}
\setcounter{lastsheet}{42}
\setcounter{lastpage}{31}
\setcounter{figure}{8}
\setcounter{lofdepth}{1}
\setcounter{table}{21}
\setcounter{lotdepth}{1}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{265}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{5}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{r@tfl@t}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{bookmark@seq@number}{14}
\setcounter{nlinenum}{0}
\setcounter{section@level}{1}
}
