\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {american}{}
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.1}{\ignorespaces Anytime inference produces a progression of outputs. \relax }}{3}{figure.caption.4}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.2}{\ignorespaces Proposed anytime dense prediction with confidence (ADP-C) approach. We equip the model with intermediate exits for anytime inference. We redesign each exit with encoder-decoder architecture to compensate for spatial resolution across model stages. At each exit's output, sufficiently confident predictions (green squares) are identified to skip further computation in the following layers. \relax }}{5}{figure.caption.6}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Accuracy and computation at four exits across architectures and tasks. Redesigned heads (RH) boost the accuracy at early exits, while confidence adaptivity (CA) reduces computation by up to more than half. ADP-C outperforms baseline methods across the accuracy-computation tradeoff curve. \relax }}{11}{figure.caption.18}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.4}{\ignorespaces Visualizations of ADP-C results. Top: prediction results at all exits. Middle: confidence maps, lighter color indicates higher confidence. Bottom: correct/wrong predictions at the exit drawn as white/black. The confident points selected for masking are in green. Confidence adaptivity excludes calculation on already confident pixels (green) in early exits, mostly located at inner parts of large segments.\relax }}{13}{figure.caption.21}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.5}{\ignorespaces Analysis of ADP-C. \emph {Left:} comparing downsampling strategies. $D=3/2/1$ means downsampling the features $3/2/1$ times at exit $1/2/3$. \emph {Right:} comparison between different masking criteria.\relax }}{14}{figure.caption.24}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.6}{\ignorespaces Input and ground truth of the example validation images visualized in Fig.~\ref {fig:vis1} and Fig.~\ref {fig:vis2}\relax }}{18}{figure.caption.35}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.7}{\ignorespaces Additional visualizations of ADP-C results, part 1. \relax }}{19}{figure.caption.36}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.8}{\ignorespaces Additional visualizations of ADP-C results, part 2. \relax }}{20}{figure.caption.37}%
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces A typical three-stage network pruning pipeline.\relax }}{22}{figure.caption.38}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Difference between predefined and automatically discovered target architectures, in channel pruning as an example. The pruning ratio $x$ is user-specified, while $a, b, c, d$ are determined by the pruning algorithm. Unstructured sparse pruning can also be viewed as automatic.\relax }}{23}{figure.caption.39}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Pruned architectures obtained by different approaches, all \emph {trained from scratch}, averaged over 5 runs. Architectures obtained by automatic pruning methods (\emph {Left:} Network Slimming~\cite {liu2017learning}, \emph {Right:} Unstructured pruning~\cite {han2015learning}) have better parameter efficiency than uniformly pruning channels or sparsifying weights in the whole network. \relax }}{32}{figure.caption.54}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.4}{\ignorespaces The average sparsity pattern of all 3$\times $3 convolutional kernels in certain layer stages in a unstructured pruned VGG-16. Darker color means higher probability of weight being kept.\relax }}{33}{table.caption.55}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.5}{\ignorespaces Pruned architectures obtained by different approaches, \emph {all trained from scratch}, averaged over 5 runs. \emph {Left:} Results for PreResNet-164 pruned on CIFAR-10 by Network Slimming~\cite {liu2017learning}. \emph {Middle} and \emph {Right}: Results for PreResNet-110 and DenseNet-40 pruned on CIFAR-100 by unstructured pruning~\cite {han2015learning}. \relax }}{33}{figure.caption.56}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.6}{\ignorespaces Pruned architectures obtained by different approaches, \emph {all trained from scratch}, averaged over 5 runs. ``Guided Pruning/Sparsification'' means using the average sparsity patterns in each layer stage to design the network; ``Transferred Guided Pruning/Sparsification'' means using the sparsity patterns obtained by a pruned VGG-16 on CIFAR-10, to design the network for VGG-19 on CIFAR-100. Following the design guidelines provided by the pruned architectures, we achieve better parameter efficiency, even when the guidelines are transferred from another dataset and model.\relax }}{34}{figure.caption.59}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.7}{\ignorespaces Comparisons with the Lottery Ticket Hypothesis~\cite {lottery} for iterative/one-shot unstructured pruning~\cite {han2015learning} with two initial learning rates 0.1 and 0.01, on CIFAR-10 dataset. Each point is averaged over 5 runs. Using the winning ticket as initialization only brings improvement when the learning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).\relax }}{36}{figure.caption.61}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.8}{\ignorespaces Weight distribution of convolutional layers for different pruning methods. We use VGG-16 and CIFAR-10 for this visualization. We compare the weight distribution of unpruned models, fine-tuned models and scratch-trained models. \emph {Top}: Results for Network Slimming~\cite {liu2017learning}. \emph {Bottom}: Results for unstructured pruning~\cite {han2015learning}.\relax }}{43}{figure.caption.78}%
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf {ImageNet-1K classification} results for \leavevmode {\color {convcolor}$\bullet $\,}ConvNets and \leavevmode {\color {vitcolor}$\mathbf {\circ }$\,}vision Transformers. Each bubble's area is proportional to FLOPs of a variant in a model family. ImageNet-1K/22K models here take 224$^2$/384$^2$ images respectively. ResNet and ViT results were obtained with improved training procedures over the original papers. We demonstrate that a standard ConvNet model can achieve the same level of scalability as hierarchical vision Transformers while being much simpler in design.\relax }}{48}{figure.caption.84}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.2}{\ignorespaces We modernize a standard ConvNet (ResNet) towards the design of a hierarchical vision Transformer (Swin), without introducing any attention-based modules. The foreground bars are model accuracies in the ResNet-50/Swin-T FLOP regime; results for the ResNet-200/Swin-B regime are shown with the gray bars. A hatched bar means the modification is not adopted. Detailed results for both regimes are in the appendix. Many Transformer architectural choices can be incorporated in a ConvNet, and they lead to increasingly better performance. In the end, our pure ConvNet model, named ConvNeXt, can outperform the Swin Transformer.\relax }}{51}{figure.caption.85}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf {Block modifications and resulted specifications.} \textbf {(a)} is a ResNeXt block; in \textbf {(b)} we create an inverted bottleneck block and in \textbf {(c)} the position of the spatial depthwise conv layer is moved up.\relax }}{54}{figure.caption.94}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.4}{\ignorespaces \textbf {Block designs} for a ResNet, a Swin Transformer, and a ConvNeXt. Swin Transformer's block is more sophisticated due to the presence of multiple specialized modules and two residual connections. For simplicity, we note the linear layers in Transformer MLP blocks also as ``1$\times $1 convs'' since they are equivalent.\relax }}{56}{figure.caption.99}%
