\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {american}{}
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.1}{\ignorespaces \textbf {ImageNet-1K classification} results for \leavevmode {\color {convcolor}$\bullet $\,}ConvNets and \leavevmode {\color {vitcolor}$\mathbf {\circ }$\,}vision Transformers. Each bubble's area is proportional to FLOPs of a variant in a model family. ImageNet-1K/22K models here take 224$^2$/384$^2$ images respectively. ResNet and ViT results were obtained with improved training procedures over the original papers. We demonstrate that a standard ConvNet model can achieve the same level of scalability as hierarchical vision Transformers while being much simpler in design.\relax }}{3}{figure.caption.4}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.2}{\ignorespaces We modernize a standard ConvNet (ResNet) towards the design of a hierarchical vision Transformer (Swin), without introducing any attention-based modules. The foreground bars are model accuracies in the ResNet-50/Swin-T FLOP regime; results for the ResNet-200/Swin-B regime are shown with the gray bars. A hatched bar means the modification is not adopted. Detailed results for both regimes are in the appendix. Many Transformer architectural choices can be incorporated in a ConvNet, and they lead to increasingly better performance. In the end, our pure ConvNet model, named ConvNeXt, can outperform the Swin Transformer.\relax }}{6}{figure.caption.5}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.3}{\ignorespaces \textbf {Block modifications and resulted specifications.} \textbf {(a)} is a ResNeXt block; in \textbf {(b)} we create an inverted bottleneck block and in \textbf {(c)} the position of the spatial depthwise conv layer is moved up.\relax }}{10}{figure.caption.14}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.4}{\ignorespaces \textbf {Block designs} for a ResNet, a Swin Transformer, and a ConvNeXt. Swin Transformer's block is more sophisticated due to the presence of multiple specialized modules and two residual connections. For simplicity, we note the linear layers in Transformer MLP blocks also as ``1$\times $1 convs'' since they are equivalent.\relax }}{11}{figure.caption.19}%
