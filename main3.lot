\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {2.1}{\ignorespaces Accuracy and inference computation for Cityscapes semantic segmentation with four exits. Our approach achieves higher accuracy in less computation than the HRNet and MDEQ baselines across exits. Early exiting (EE) makes progressive predictions. Redesigned heads (RH) improve early predictions (exits 1 and 2). Confidence Adaptivity (CA) reduces computation. \relax }}{11}{table.caption.9}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {2.2}{\ignorespaces Accuracy and computation on MPII pose estimation at the last exit and averaged for all exits. Early exits (EE) make progress predictions, redesigned heads (RH) improve accuracy, and confidence adaptivity (CA) reduces computation. \relax }}{13}{table.caption.11}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {2.3}{\ignorespaces Accuracy and inference computation for Cityscapes semantic segmentation with four exits under different settings of interpolation radius. A minor increase in mIoU and GFLOPs is observed with a larger radius. \relax }}{16}{table.caption.14}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {2.4}{\ignorespaces Accuracy and inference computation for Cityscapes semantic segmentation with four exits under different adaptivity settings. \relax }}{16}{table.caption.15}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {2.5}{\ignorespaces Accuracy and inference computation for PASCAL-Context semantic segmentation. \relax }}{17}{table.caption.16}%
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.1}{\ignorespaces Results (accuracy \% by default) for $L_1$-norm based filter pruning\nobreakspace {}\cite {li2016pruning}. ``Pruned Model'' is the model pruned from the large model. Configurations of Model and Pruned Model are both from the original paper. \relax }}{28}{table.caption.22}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.2}{\ignorespaces Results for ThiNet\nobreakspace {}\cite {luo2017thinet}. Names such as ``VGG-GAP'' and ``ResNet50-30\%'' are pruned models whose configurations are defined in\nobreakspace {}\cite {luo2017thinet}. To accommodate the effects of different frameworks between our implementation and the original paper's, we compare relative accuracy drop from the unpruned large model. For example, for the pruned model VGG-Conv, $-$1.23 is relative to 71.03 on the left, which is the reported accuracy of the unpruned large model VGG-16 in the original paper; $-$2.75 is relative to 71.51 on the left, which is VGG-16's accuracy in our implementation. \relax }}{28}{table.caption.23}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.3}{\ignorespaces Results for Regression based Feature Reconstruction\nobreakspace {}\cite {he2017channel}. Pruned models such as ``VGG-16-5$\times $'' are defined in\nobreakspace {}\cite {he2017channel}. Similar to Table\nobreakspace {}\ref {thinet}, we compare relative accuracy drop from unpruned large models.\relax }}{29}{table.caption.24}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.4}{\ignorespaces Results for Network Slimming\nobreakspace {}\cite {liu2017learning}. ``Prune ratio'' stands for total percentage of channels that are pruned in the whole network. The same ratios for each model are used as the original paper. \relax }}{30}{table.caption.25}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.5}{\ignorespaces Results for residual block pruning using Sparse Structure Selection\nobreakspace {}\cite {huang2018data}. In the original paper no fine-tuning is required so there is a ``Pruned'' column instead of ``Fine-tuned'' as before. \relax }}{30}{table.caption.26}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.6}{\ignorespaces Results for unstructured pruning\nobreakspace {}\cite {han2015learning}. ``Prune Ratio'' denotes the percentage of parameters pruned in the set of all convolutional weights. \relax }}{31}{table.caption.27}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.7}{\ignorespaces Network architectures obtained by pruning 60\% channels on VGG-16 (in total 13 conv-layers) using Network Slimming. Width and Width* are number of channels in the original and pruned architectures, averaged over 5 runs.\relax }}{33}{table.caption.29}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.8}{\ignorespaces Comparisons with the Lottery Ticket Hypothesis on a structured pruning method ($L_1$-norm based filter pruning\nobreakspace {}\cite {li2016pruning}) with two initial learning rates: 0.1 and 0.01. In both cases, using winning tickets does not bring improvement on accuracy.\relax }}{37}{table.caption.33}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.9}{\ignorespaces Comparisons with the Lottery Ticket Hypothesis for one-shot unstructured pruning\nobreakspace {}\cite {han2015learning} with two initial learning rates: 0.1 and 0.01. The same results are visualized in Figure\nobreakspace {}\ref {iterative-3}. Using the winning ticket as initialization only brings improvement when the learning rate is small (0.01), however such small learning rate leads to a lower accuracy than the widely used large learning rate (0.1).\relax }}{38}{table.caption.34}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.10}{\ignorespaces Results for Soft Filter Pruning\nobreakspace {}\cite {he2018sfp} without pretrained models.\relax }}{39}{table.caption.35}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.11}{\ignorespaces Results for Soft Filter Pruning\nobreakspace {}\cite {he2018sfp} using pretrained models.\relax }}{40}{table.caption.36}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.12}{\ignorespaces Results (mAP) for pruning on detection task. The pruned models are chosen from\nobreakspace {}\cite {li2016pruning}. Prune-C refers to pruning on classifcation pre-trained weights, Prune-D refers to pruning after the weights are transferred to detection task. Scratch-E/B means pre-training the pruned model from scratch on classification and transfer to detection.\relax }}{40}{table.caption.37}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.13}{\ignorespaces Results for Network Slimming\nobreakspace {}\cite {liu2017learning} when the models are aggressively pruned. ``Prune ratio'' stands for total percentage of channels that are pruned in the whole network. Larger ratios are used than the original paper of\nobreakspace {}\cite {liu2017learning}. \relax }}{41}{table.caption.38}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.14}{\ignorespaces Results for $L_1$-norm based filter pruning\nobreakspace {}\cite {li2016pruning} when the models are aggressively pruned. \relax }}{41}{table.caption.39}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.15}{\ignorespaces Results for unstructured pruning\nobreakspace {}\cite {han2015learning} when the models are aggressively pruned. \relax }}{42}{table.caption.40}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.16}{\ignorespaces Results for extending fine-tuning. ``Fine-tune-40'' stands for fine-tuning 40 epochs and so on. Scratch-E models are trained for 160 epochs. We observe that fine-tuning for more epochs does not help improve the accuracy much, and models trained from scratch can still perform on par with fine-tuned models. \relax }}{42}{table.caption.41}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.17}{\ignorespaces Results for $L_1$-norm filter pruning\nobreakspace {}\cite {li2016pruning} when the training schedule of the large model is extended from 160 to 300 epochs. \relax }}{43}{table.caption.42}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.18}{\ignorespaces Sparsity patterns of PreResNet-164 pruned on CIFAR-10 by Network Slimming shown in Figure\nobreakspace {}\ref {arch-search-negative} (left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channels to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform across stages.\relax }}{44}{table.caption.44}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.19}{\ignorespaces Average sparsity patterns of 3$\times $3 kernels of PreResNet-110 pruned on CIFAR-100 by unstructured pruning shown in Figure\nobreakspace {}\ref {arch-search-negative} (middle) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform across stages.\relax }}{45}{table.caption.45}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.20}{\ignorespaces Average sparsity patterns of 3$\times $3 kernels of DenseNet-40 pruned on CIFAR-100 by unstructured pruning shown in Figure\nobreakspace {}\ref {arch-search-negative} (right) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of weights to be kept. We can observe that for a certain prune ratio, the sparsity patterns are close to uniform across stages.\relax }}{45}{table.caption.46}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {3.21}{\ignorespaces Sparsity patterns of VGG-16 pruned on CIFAR-10 by Network Slimming shown in Figure\nobreakspace {}\ref {arch-search} (left) under different prune ratio. The top row denotes the total prune ratio. The values denote the ratio of channels to be kept. For each prune ratio, the latter stages tend to have more redundancy than earlier stages.\relax }}{46}{table.caption.47}%
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.1}{\ignorespaces Classification accuracy on ImageNet-1K. Similar to Transformers, ConvNeXt{} also shows promising scaling behavior with higher-capacity models and a larger (pre-training) dataset. Inference throughput is measured on a V100 GPU, following\nobreakspace {}\cite {Liu2021swin}. On an A100 GPU, ConvNeXt{} can have a much higher throughput than Swin Transformer. See Section\nobreakspace {}\ref {sec:a100}. ({\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 84}) ViT results with 90-epoch AugReg\nobreakspace {}\cite {steiner2021train} training, provided through personal communication with the authors.\relax }}{60}{table.caption.65}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.2}{\ignorespaces Comparing isotropic ConvNeXt{} and ViT. Training memory is measured on V100 GPUs with 32 per-GPU batch size.\relax }}{61}{table.caption.68}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.3}{\ignorespaces COCO object detection and segmentation results using Mask-RCNN and Cascade Mask-RCNN. $^\ddag $ indicates that the model is pre-trained on ImageNet-22K. ImageNet-1K pre-trained Swin results are from their Github repository\nobreakspace {}\cite {swindetcode}. AP numbers of the ResNet-50 and X101 models are from \cite {Liu2021swin}. We measure FPS on an A100 GPU. FLOPs are calculated with image size (1280, 800). \relax }}{62}{table.caption.69}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.4}{\ignorespaces ADE20K validation results using UperNet\nobreakspace {}\cite {Xiao2018}. $^\ddag $ indicates IN-22K pre-training. Swins' results are from its GitHub repository\nobreakspace {}\cite {swincode}. Following Swin, we report mIoU results with multi-scale testing. FLOPs are based on input sizes of (2048, 512) and (2560, 640) for IN-1K and IN-22K pre-trained models, respectively.\relax }}{63}{table.caption.70}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.5}{\ignorespaces ImageNet-1K/22K (pre-)training settings. Multiple stochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model (e.g., ConvNeXt{}-T/S/B/L) respectively.\relax }}{65}{table.caption.71}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.6}{\ignorespaces ImageNet-1K fine-tuning settings. Multiple values (e.g., 0.8/0.95) are for each model (e.g., ConvNeXt{}-B/L) respectively. \relax }}{66}{table.caption.72}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.7}{\ignorespaces ADE20K validation results with single-scale testing.\relax }}{67}{table.caption.73}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.8}{\ignorespaces Detailed architecture specifications for ResNet-50, ConvNeXt{}-T and Swin-T.\relax }}{68}{table.caption.74}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.9}{\ignorespaces Robustness evaluation of ConvNeXt. We do not make use of any specialized modules or additional fine-tuning procedures. \relax }}{69}{table.caption.75}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.10}{\ignorespaces Detailed results for modernizing a ResNet-50. Mean and standard deviation are obtained by training the network with three different random seeds.\relax }}{70}{table.caption.76}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.11}{\ignorespaces Detailed results for modernizing a ResNet-200.\relax }}{71}{table.caption.77}%
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {4.12}{\ignorespaces Inference throughput comparisons on an A100 GPU. Using TF32 data format and ``channel last'' memory layout, ConvNeXt{} enjoys up to $\sim $49\% higher throughput compared with a Swin Transformer with similar FLOPs.\relax }}{72}{table.caption.78}%
\defcounter {refsection}{0}\relax 
\addvspace {10pt}
