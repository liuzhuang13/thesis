\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}A ConvNet for the 2020s}{2}{chapter.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Overview}{2}{section.2.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Introduction}{3}{section.2.2}\protected@file@percent }
\newlabel{sec:intro}{{\M@TitleReference {2.2}{Introduction}}{3}{Introduction}{section.2.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \textbf  {ImageNet-1K classification} results for \leavevmode {\color  {convcolor}$\bullet $\,}ConvNets and \leavevmode {\color  {vitcolor}$\mathbf  {\circ }$\,}vision Transformers. Each bubble's area is proportional to FLOPs of a variant in a model family. ImageNet-1K/22K models here take 224$^2$/384$^2$ images respectively. ResNet and ViT results were obtained with improved training procedures over the original papers. We demonstrate that a standard ConvNet model can achieve the same level of scalability as hierarchical vision Transformers while being much simpler in design.\relax }}{3}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser}{{\M@TitleReference {2.1}{\textbf  {ImageNet-1K classification} results for \leavevmode {\color  {convcolor}$\bullet $\,}ConvNets and \leavevmode {\color  {vitcolor}$\mathbf  {\circ }$\,}vision Transformers. Each bubble's area is proportional to FLOPs of a variant in a model family. ImageNet-1K/22K models here take 224$^2$/384$^2$ images respectively. ResNet and ViT results were obtained with improved training procedures over the original papers. We demonstrate that a standard ConvNet model can achieve the same level of scalability as hierarchical vision Transformers while being much simpler in design.\relax }}{3}{\textbf {ImageNet-1K classification} results for \cb ConvNets and \vb vision Transformers. Each bubble's area is proportional to FLOPs of a variant in a model family. ImageNet-1K/22K models here take 224$^2$/384$^2$ images respectively. ResNet and ViT results were obtained with improved training procedures over the original papers. We demonstrate that a standard ConvNet model can achieve the same level of scalability as hierarchical vision Transformers while being much simpler in design.\relax }{figure.caption.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Modernizing a ConvNet: a Roadmap}{5}{section.2.3}\protected@file@percent }
\newlabel{sec:modernizing}{{\M@TitleReference {2.3}{Modernizing a ConvNet: a Roadmap}}{5}{Modernizing a ConvNet: a Roadmap}{section.2.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces We modernize a standard ConvNet (ResNet) towards the design of a hierarchical vision Transformer (Swin), without introducing any attention-based modules. The foreground bars are model accuracies in the ResNet-50/Swin-T FLOP regime; results for the ResNet-200/Swin-B regime are shown with the gray bars. A hatched bar means the modification is not adopted. Detailed results for both regimes are in the appendix. Many Transformer architectural choices can be incorporated in a ConvNet, and they lead to increasingly better performance. In the end, our pure ConvNet model, named ConvNeXt, can outperform the Swin Transformer.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:morph_main}{{\M@TitleReference {2.2}{We modernize a standard ConvNet (ResNet) towards the design of a hierarchical vision Transformer (Swin), without introducing any attention-based modules. The foreground bars are model accuracies in the ResNet-50/Swin-T FLOP regime; results for the ResNet-200/Swin-B regime are shown with the gray bars. A hatched bar means the modification is not adopted. Detailed results for both regimes are in the appendix. Many Transformer architectural choices can be incorporated in a ConvNet, and they lead to increasingly better performance. In the end, our pure ConvNet model, named ConvNeXt, can outperform the Swin Transformer.\relax }}{6}{We modernize a standard ConvNet (ResNet) towards the design of a hierarchical vision Transformer (Swin), without introducing any attention-based modules. The foreground bars are model accuracies in the ResNet-50/Swin-T FLOP regime; results for the ResNet-200/Swin-B regime are shown with the gray bars. A hatched bar means the modification is not adopted. Detailed results for both regimes are in the appendix. Many Transformer architectural choices can be incorporated in a ConvNet, and they lead to increasingly better performance. In the end, our pure ConvNet model, named ConvNeXt, can outperform the Swin Transformer.\relax }{figure.caption.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Training Techniques}{7}{section*.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Macro Design}{7}{section*.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Changing stage compute ratio.}{7}{section*.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Changing stem to ``Patchify''.}{8}{section*.9}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{ResNeXt-ify}{8}{section*.10}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Inverted Bottleneck}{9}{section*.11}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Large Kernel Sizes}{9}{section*.12}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Moving up depthwise conv layer.}{9}{figure.caption.14}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Increasing the kernel size.}{9}{section*.15}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \textbf  {Block modifications and resulted specifications.} \textbf  {(a)} is a ResNeXt block; in \textbf  {(b)} we create an inverted bottleneck block and in \textbf  {(c)} the position of the spatial depthwise conv layer is moved up.\relax }}{10}{figure.caption.14}\protected@file@percent }
\newlabel{fig:inverted}{{\M@TitleReference {2.3}{\textbf  {Block modifications and resulted specifications.} \textbf  {(a)} is a ResNeXt block; in \textbf  {(b)} we create an inverted bottleneck block and in \textbf  {(c)} the position of the spatial depthwise conv layer is moved up.\relax }}{10}{\textbf {Block modifications and resulted specifications.} \textbf {(a)} is a ResNeXt block; in \textbf {(b)} we create an inverted bottleneck block and in \textbf {(c)} the position of the spatial depthwise conv layer is moved up.\relax }{figure.caption.14}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Micro Design}{10}{section*.16}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Replacing ReLU with GELU}{10}{section*.17}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Fewer activation functions.}{10}{section*.18}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \textbf  {Block designs} for a ResNet, a Swin Transformer, and a ConvNeXt. Swin Transformer's block is more sophisticated due to the presence of multiple specialized modules and two residual connections. For simplicity, we note the linear layers in Transformer MLP blocks also as ``1$\times $1 convs'' since they are equivalent.\relax }}{11}{figure.caption.19}\protected@file@percent }
\newlabel{fig:block}{{\M@TitleReference {2.4}{\textbf  {Block designs} for a ResNet, a Swin Transformer, and a ConvNeXt. Swin Transformer's block is more sophisticated due to the presence of multiple specialized modules and two residual connections. For simplicity, we note the linear layers in Transformer MLP blocks also as ``1$\times $1 convs'' since they are equivalent.\relax }}{11}{\textbf {Block designs} for a ResNet, a Swin Transformer, and a \cnn . Swin Transformer's block is more sophisticated due to the presence of multiple specialized modules and two residual connections. For simplicity, we note the linear layers in Transformer MLP blocks also as ``1$\times $1 convs'' since they are equivalent.\relax }{figure.caption.19}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Fewer normalization layers.}{12}{section*.20}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Substituting BN with LN.}{12}{section*.21}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Separate downsampling layers.}{12}{section*.22}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Closing remarks.}{12}{section*.23}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}Empirical Evaluations on ImageNet}{13}{section.2.4}\protected@file@percent }
\newlabel{sec:convnext_config}{{\M@TitleReference {2.4}{Empirical Evaluations on ImageNet}}{13}{Empirical Evaluations on ImageNet}{section.2.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Settings}{13}{section*.24}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Training on ImageNet-1K.}{13}{section*.25}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Pre-training on ImageNet-22K.}{14}{section*.26}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Fine-tuning on ImageNet-1K.}{14}{section*.27}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Results}{14}{section*.29}\protected@file@percent }
\newlabel{subsec:imagenet-results}{{\M@TitleReference {2.4}{Results}}{14}{Results}{section*.29}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{ImageNet-1K.}{14}{section*.30}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{ImageNet-22K.}{14}{section*.31}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces \textbf  {Classification accuracy on ImageNet-1K.} Similar to Transformers, ConvNeXt{} also shows promising scaling behavior with higher-capacity models and a larger (pre-training) dataset. Inference throughput is measured on a V100 GPU, following~\cite {Liu2021swin}. On an A100 GPU, ConvNeXt{} can have a much higher throughput than Swin Transformer. See Appendix~\ref  {sec:a100}. ({\fontfamily  {mvs}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 84})ViT results with 90-epoch AugReg~\cite {steiner2021train} training, provided through personal communication with the authors.\relax }}{15}{table.caption.28}\protected@file@percent }
\newlabel{tab:imagenet-system}{{\M@TitleReference {2.1}{\textbf  {Classification accuracy on ImageNet-1K.} Similar to Transformers, ConvNeXt{} also shows promising scaling behavior with higher-capacity models and a larger (pre-training) dataset. Inference throughput is measured on a V100 GPU, following~\cite {Liu2021swin}. On an A100 GPU, ConvNeXt{} can have a much higher throughput than Swin Transformer. See Appendix~\ref  {sec:a100}. ({\fontfamily  {mvs}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 84})ViT results with 90-epoch AugReg~\cite {steiner2021train} training, provided through personal communication with the authors.\relax }}{15}{\textbf {Classification accuracy on ImageNet-1K.} Similar to Transformers, \cnn {} also shows promising scaling behavior with higher-capacity models and a larger (pre-training) dataset. Inference throughput is measured on a V100 GPU, following~\cite {Liu2021swin}. On an A100 GPU, \cnn {} can have a much higher throughput than Swin Transformer. See Appendix~\ref {sec:a100}. (\Telefon )ViT results with 90-epoch AugReg~\cite {steiner2021train} training, provided through personal communication with the authors.\relax }{table.caption.28}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Isotropic ConvNeXt{} \emph  {vs}\onedot  ViT}{16}{section*.32}\protected@file@percent }
\newlabel{subsec:isotropic}{{\M@TitleReference {2.4}{Isotropic ConvNeXt{} \emph  {vs}\onedot  ViT}}{16}{Isotropic \cnn {} \vs ViT}{section*.32}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces \textbf  {Comparing isotropic ConvNeXt{} and ViT.} Training memory is measured on V100 GPUs with 32 per-GPU batch size.\relax }}{16}{table.caption.33}\protected@file@percent }
\newlabel{tab:non-hie}{{\M@TitleReference {2.2}{\textbf  {Comparing isotropic ConvNeXt{} and ViT.} Training memory is measured on V100 GPUs with 32 per-GPU batch size.\relax }}{16}{\textbf {Comparing isotropic \cnn {} and ViT.} Training memory is measured on V100 GPUs with 32 per-GPU batch size.\relax }{table.caption.33}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.5}Empirical Evaluation on Downstream Tasks}{16}{section.2.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Object detection and segmentation on COCO.}{16}{section*.34}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces caption}}{17}{table.caption.35}\protected@file@percent }
\newlabel{tab:coco}{{\M@TitleReference {2.3}{caption}}{17}{caption}{table.caption.35}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Semantic segmentation on ADE20K.}{17}{section*.36}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Remarks on model efficiency.}{17}{section*.38}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces caption}}{18}{table.caption.37}\protected@file@percent }
\newlabel{tab:seg}{{\M@TitleReference {2.4}{caption}}{18}{caption}{table.caption.37}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.6}Related Work}{18}{section.2.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Hybrid models.}{18}{section*.39}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Recent convolution-based approaches.}{19}{section*.40}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.7}Conclusions}{19}{section.2.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Acknowledgments.}{19}{section*.41}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.8}Experimental Settings}{19}{section.2.8}\protected@file@percent }
\newlabel{sec:setting}{{\M@TitleReference {2.8}{Experimental Settings}}{19}{Experimental Settings}{section.2.8}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{ImageNet (Pre-)training}{19}{section*.42}\protected@file@percent }
\newlabel{subsec:setting}{{\M@TitleReference {2.8}{ImageNet (Pre-)training}}{19}{ImageNet (Pre-)training}{section*.42}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces \textbf  {ImageNet-1K/22K (pre-)training settings}. Multiple stochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model (e.g., ConvNeXt{}-T/S/B/L) respectively.\relax }}{20}{table.caption.43}\protected@file@percent }
\newlabel{tab:train_detail}{{\M@TitleReference {2.5}{\textbf  {ImageNet-1K/22K (pre-)training settings}. Multiple stochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model (e.g., ConvNeXt{}-T/S/B/L) respectively.\relax }}{20}{\textbf {ImageNet-1K/22K (pre-)training settings}. Multiple stochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model (e.g., \cnn {}-T/S/B/L) respectively.\relax }{table.caption.43}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces \textbf  {ImageNet-1K fine-tuning settings}. Multiple values (e.g., 0.8/0.95) are for each model (e.g., ConvNeXt{}-B/L) respectively. \relax }}{21}{table.caption.44}\protected@file@percent }
\newlabel{tab:ft_detail}{{\M@TitleReference {2.6}{\textbf  {ImageNet-1K fine-tuning settings}. Multiple values (e.g., 0.8/0.95) are for each model (e.g., ConvNeXt{}-B/L) respectively. \relax }}{21}{\textbf {ImageNet-1K fine-tuning settings}. Multiple values (e.g., 0.8/0.95) are for each model (e.g., \cnn {}-B/L) respectively. \relax }{table.caption.44}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{ImageNet Fine-tuning}{22}{section*.45}\protected@file@percent }
\newlabel{subsec:ft-setting}{{\M@TitleReference {2.8}{ImageNet Fine-tuning}}{22}{ImageNet Fine-tuning}{section*.45}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{Downstream Tasks}{22}{section*.46}\protected@file@percent }
\newlabel{subsec:downstream-setting}{{\M@TitleReference {2.8}{Downstream Tasks}}{22}{Downstream Tasks}{section*.46}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.7}{\ignorespaces caption}}{22}{table.caption.47}\protected@file@percent }
\newlabel{tab:seg-ss}{{\M@TitleReference {2.7}{caption}}{22}{caption}{table.caption.47}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.9}Robustness Evaluation}{23}{section.2.9}\protected@file@percent }
\newlabel{sec:robustness}{{\M@TitleReference {2.9}{Robustness Evaluation}}{23}{Robustness Evaluation}{section.2.9}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.8}{\ignorespaces caption}}{23}{table.caption.48}\protected@file@percent }
\newlabel{tab:robustness}{{\M@TitleReference {2.8}{caption}}{23}{caption}{table.caption.48}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.10}Modernizing ResNets: detailed results}{23}{section.2.10}\protected@file@percent }
\newlabel{sec:modernizing_result}{{\M@TitleReference {2.10}{Modernizing ResNets: detailed results}}{23}{Modernizing ResNets: detailed results}{section.2.10}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.9}{\ignorespaces \textbf  {Detailed architecture specifications} for ResNet-50, ConvNeXt{}-T and Swin-T.\relax }}{24}{table.caption.49}\protected@file@percent }
\newlabel{table:arch-spec}{{\M@TitleReference {2.9}{\textbf  {Detailed architecture specifications} for ResNet-50, ConvNeXt{}-T and Swin-T.\relax }}{24}{\textbf {Detailed architecture specifications} for ResNet-50, \cnn {}-T and Swin-T.\relax }{table.caption.49}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.11}Detailed Architectures}{24}{section.2.11}\protected@file@percent }
\newlabel{sec:arch}{{\M@TitleReference {2.11}{Detailed Architectures}}{24}{Detailed Architectures}{section.2.11}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.10}{\ignorespaces \textbf  {Detailed results for modernizing a ResNet-50.} Mean and standard deviation are obtained by training the network with three different random seeds.\relax }}{25}{table.caption.50}\protected@file@percent }
\newlabel{tab:modernizing-t}{{\M@TitleReference {2.10}{\textbf  {Detailed results for modernizing a ResNet-50.} Mean and standard deviation are obtained by training the network with three different random seeds.\relax }}{25}{\textbf {Detailed results for modernizing a ResNet-50.} Mean and standard deviation are obtained by training the network with three different random seeds.\relax }{table.caption.50}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.12}Benchmarking on A100 GPUs}{25}{section.2.12}\protected@file@percent }
\newlabel{sec:a100}{{\M@TitleReference {2.12}{Benchmarking on A100 GPUs}}{25}{Benchmarking on A100 GPUs}{section.2.12}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.11}{\ignorespaces \textbf  {Detailed results for modernizing a ResNet-200.}\relax }}{26}{table.caption.51}\protected@file@percent }
\newlabel{tab:modernizing-b}{{\M@TitleReference {2.11}{\textbf  {Detailed results for modernizing a ResNet-200.}\relax }}{26}{\textbf {Detailed results for modernizing a ResNet-200.}\relax }{table.caption.51}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.13}Limitations}{26}{section.2.13}\protected@file@percent }
\newlabel{sec:limit}{{\M@TitleReference {2.13}{Limitations}}{26}{Limitations}{section.2.13}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.14}Societal Impact}{26}{section.2.14}\protected@file@percent }
\newlabel{sec:impact}{{\M@TitleReference {2.14}{Societal Impact}}{26}{Societal Impact}{section.2.14}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2.12}{\ignorespaces \textbf  {Inference throughput comparisons on an A100 GPU.} Using TF32 data format and ``channel last'' memory layout, ConvNeXt{} enjoys up to $\sim $49\% higher throughput compared with a Swin Transformer with similar FLOPs.\relax }}{27}{table.caption.52}\protected@file@percent }
\newlabel{tab:a100}{{\M@TitleReference {2.12}{\textbf  {Inference throughput comparisons on an A100 GPU.} Using TF32 data format and ``channel last'' memory layout, ConvNeXt{} enjoys up to $\sim $49\% higher throughput compared with a Swin Transformer with similar FLOPs.\relax }}{27}{\textbf {Inference throughput comparisons on an A100 GPU.} Using TF32 data format and ``channel last'' memory layout, \cnn {} enjoys up to $\sim $49\% higher throughput compared with a Swin Transformer with similar FLOPs.\relax }{table.caption.52}{}}
\@setckpt{chap4}{
\setcounter{page}{28}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{-1}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{14}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{1}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{36}
\setcounter{lastsheet}{35}
\setcounter{lastpage}{27}
\setcounter{figure}{4}
\setcounter{lofdepth}{1}
\setcounter{table}{12}
\setcounter{lotdepth}{1}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{0}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{r@tfl@t}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{bookmark@seq@number}{19}
\setcounter{nlinenum}{0}
\setcounter{section@level}{1}
}
