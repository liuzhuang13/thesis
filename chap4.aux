\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}A ConvNet for the 2020s}{47}{chapter.4}\protected@file@percent }
\newlabel{chap:convnext}{{\M@TitleReference {4}{A ConvNet for the 2020s}}{47}{A ConvNet for the 2020s}{chapter.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.1}Overview}{47}{section.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.2}Introduction}{48}{section.4.2}\protected@file@percent }
\newlabel{sec:intro}{{\M@TitleReference {4.2}{Introduction}}{48}{Introduction}{section.4.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces ImageNet-1K classification results for \leavevmode {\color  {convcolor}$\bullet $\,}ConvNets and \leavevmode {\color  {vitcolor}$\mathbf  {\circ }$\,}vision Transformers. Each bubble's area is proportional to FLOPs of a variant in a model family. ImageNet-1K/22K models here take 224$^2$/384$^2$ images respectively. ResNet and ViT results were obtained with improved training procedures over the original papers. We demonstrate that a standard ConvNet model can achieve the same level of scalability as hierarchical vision Transformers while being much simpler in design.\relax }}{49}{figure.caption.48}\protected@file@percent }
\newlabel{fig:teaser_convnext}{{\M@TitleReference {4.1}{ImageNet-1K classification results for \leavevmode {\color  {convcolor}$\bullet $\,}ConvNets and \leavevmode {\color  {vitcolor}$\mathbf  {\circ }$\,}vision Transformers. Each bubble's area is proportional to FLOPs of a variant in a model family. ImageNet-1K/22K models here take 224$^2$/384$^2$ images respectively. ResNet and ViT results were obtained with improved training procedures over the original papers. We demonstrate that a standard ConvNet model can achieve the same level of scalability as hierarchical vision Transformers while being much simpler in design.\relax }}{49}{ImageNet-1K classification results for \cb ConvNets and \vb vision Transformers. Each bubble's area is proportional to FLOPs of a variant in a model family. ImageNet-1K/22K models here take 224$^2$/384$^2$ images respectively. ResNet and ViT results were obtained with improved training procedures over the original papers. We demonstrate that a standard ConvNet model can achieve the same level of scalability as hierarchical vision Transformers while being much simpler in design.\relax }{figure.caption.48}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.3}Modernizing a ConvNet: a Roadmap}{50}{section.4.3}\protected@file@percent }
\newlabel{sec:modernizing}{{\M@TitleReference {4.3}{Modernizing a ConvNet: a Roadmap}}{50}{Modernizing a ConvNet: a Roadmap}{section.4.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces We modernize a standard ConvNet (ResNet) towards the design of a hierarchical vision Transformer (Swin), without introducing any attention-based modules. The foreground bars are model accuracies in the ResNet-50/Swin-T FLOP regime; results for the ResNet-200/Swin-B regime are shown with the gray bars. A hatched bar means the modification is not adopted. Detailed results for both regimes are in Section\nobreakspace  {}\ref  {sec:modernizing}. Many Transformer architectural choices can be incorporated in a ConvNet, and they lead to increasingly better performance. In the end, our pure ConvNet model, named ConvNeXt, can outperform the Swin Transformer.\relax }}{51}{figure.caption.49}\protected@file@percent }
\newlabel{fig:morph_main}{{\M@TitleReference {4.2}{We modernize a standard ConvNet (ResNet) towards the design of a hierarchical vision Transformer (Swin), without introducing any attention-based modules. The foreground bars are model accuracies in the ResNet-50/Swin-T FLOP regime; results for the ResNet-200/Swin-B regime are shown with the gray bars. A hatched bar means the modification is not adopted. Detailed results for both regimes are in Section\nobreakspace  {}\ref  {sec:modernizing}. Many Transformer architectural choices can be incorporated in a ConvNet, and they lead to increasingly better performance. In the end, our pure ConvNet model, named ConvNeXt, can outperform the Swin Transformer.\relax }}{51}{We modernize a standard ConvNet (ResNet) towards the design of a hierarchical vision Transformer (Swin), without introducing any attention-based modules. The foreground bars are model accuracies in the ResNet-50/Swin-T FLOP regime; results for the ResNet-200/Swin-B regime are shown with the gray bars. A hatched bar means the modification is not adopted. Detailed results for both regimes are in Section~\ref {sec:modernizing}. Many Transformer architectural choices can be incorporated in a ConvNet, and they lead to increasingly better performance. In the end, our pure ConvNet model, named ConvNeXt, can outperform the Swin Transformer.\relax }{figure.caption.49}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Training Techniques}{52}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Macro Design}{52}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Changing stage compute ratio.}{52}{section*.50}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Changing stem to ``Patchify''.}{53}{section*.51}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}ResNeXt-ify}{53}{subsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Inverted Bottleneck}{54}{subsection.4.3.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Large Kernel Sizes}{54}{subsection.4.3.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Moving up depthwise conv layer.}{54}{figure.caption.53}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Increasing the kernel size.}{54}{section*.54}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Block modifications and resulted specifications. (a) is a ResNeXt block; in (b) we create an inverted bottleneck block and in (c) the position of the spatial depthwise conv layer is moved up.\relax }}{55}{figure.caption.53}\protected@file@percent }
\newlabel{fig:inverted}{{\M@TitleReference {4.3}{Block modifications and resulted specifications. (a) is a ResNeXt block; in (b) we create an inverted bottleneck block and in (c) the position of the spatial depthwise conv layer is moved up.\relax }}{55}{Block modifications and resulted specifications. (a) is a ResNeXt block; in (b) we create an inverted bottleneck block and in (c) the position of the spatial depthwise conv layer is moved up.\relax }{figure.caption.53}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Micro Design}{55}{subsection.4.3.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Replacing ReLU with GELU}{55}{section*.55}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Fewer activation functions.}{55}{section*.56}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Block designs for a ResNet, a Swin Transformer, and a ConvNeXt. Swin Transformer's block is more sophisticated due to the presence of multiple specialized modules and two residual connections. For simplicity, we note the linear layers in Transformer MLP blocks also as ``1$\times $1 convs'' since they are equivalent.\relax }}{56}{figure.caption.57}\protected@file@percent }
\newlabel{fig:block}{{\M@TitleReference {4.4}{Block designs for a ResNet, a Swin Transformer, and a ConvNeXt. Swin Transformer's block is more sophisticated due to the presence of multiple specialized modules and two residual connections. For simplicity, we note the linear layers in Transformer MLP blocks also as ``1$\times $1 convs'' since they are equivalent.\relax }}{56}{Block designs for a ResNet, a Swin Transformer, and a \cnn . Swin Transformer's block is more sophisticated due to the presence of multiple specialized modules and two residual connections. For simplicity, we note the linear layers in Transformer MLP blocks also as ``1$\times $1 convs'' since they are equivalent.\relax }{figure.caption.57}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Fewer normalization layers.}{56}{section*.58}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Substituting BN with LN.}{57}{section*.59}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Separate downsampling layers.}{57}{section*.60}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Closing remarks.}{57}{section*.61}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.4}Experiments on ImageNet}{58}{section.4.4}\protected@file@percent }
\newlabel{sec:convnext_config}{{\M@TitleReference {4.4}{Experiments on ImageNet}}{58}{Experiments on ImageNet}{section.4.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Settings}{58}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Training on ImageNet-1K.}{58}{section*.62}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Pre-training on ImageNet-22K.}{59}{section*.63}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Fine-tuning on ImageNet-1K.}{59}{section*.64}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Results}{59}{subsection.4.4.2}\protected@file@percent }
\newlabel{subsec:imagenet-results}{{\M@TitleReference {4.4.2}{Results}}{59}{Results}{subsection.4.4.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{ImageNet-1K.}{59}{section*.66}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{ImageNet-22K.}{59}{section*.67}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Classification accuracy on ImageNet-1K. Similar to Transformers, ConvNeXt{} also shows promising scaling behavior with higher-capacity models and a larger (pre-training) dataset. Inference throughput is measured on a V100 GPU, following\nobreakspace  {}\cite {Liu2021swin}. On an A100 GPU, ConvNeXt{} can have a much higher throughput than Swin Transformer. See Section\nobreakspace  {}\ref  {sec:a100}. ({\fontfamily  {mvs}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 84}) ViT results with 90-epoch AugReg\nobreakspace  {}\cite {steiner2021train} training, provided through personal communication with the authors.\relax }}{60}{table.caption.65}\protected@file@percent }
\newlabel{tab:imagenet-system}{{\M@TitleReference {4.1}{Classification accuracy on ImageNet-1K. Similar to Transformers, ConvNeXt{} also shows promising scaling behavior with higher-capacity models and a larger (pre-training) dataset. Inference throughput is measured on a V100 GPU, following\nobreakspace  {}\cite {Liu2021swin}. On an A100 GPU, ConvNeXt{} can have a much higher throughput than Swin Transformer. See Section\nobreakspace  {}\ref  {sec:a100}. ({\fontfamily  {mvs}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 84}) ViT results with 90-epoch AugReg\nobreakspace  {}\cite {steiner2021train} training, provided through personal communication with the authors.\relax }}{60}{Classification accuracy on ImageNet-1K. Similar to Transformers, \cnn {} also shows promising scaling behavior with higher-capacity models and a larger (pre-training) dataset. Inference throughput is measured on a V100 GPU, following~\cite {Liu2021swin}. On an A100 GPU, \cnn {} can have a much higher throughput than Swin Transformer. See Section~\ref {sec:a100}. (\Telefon ) ViT results with 90-epoch AugReg~\cite {steiner2021train} training, provided through personal communication with the authors.\relax }{table.caption.65}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Isotropic ConvNeXt{} \emph  {vs}\onedot  ViT}{61}{subsection.4.4.3}\protected@file@percent }
\newlabel{subsec:isotropic}{{\M@TitleReference {4.4.3}{Isotropic ConvNeXt{} \emph  {vs}\onedot  ViT}}{61}{Isotropic \cnn {} \vs ViT}{subsection.4.4.3}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Comparing isotropic ConvNeXt{} and ViT. Training memory is measured on V100 GPUs with 32 per-GPU batch size.\relax }}{61}{table.caption.68}\protected@file@percent }
\newlabel{tab:non-hie}{{\M@TitleReference {4.2}{Comparing isotropic ConvNeXt{} and ViT. Training memory is measured on V100 GPUs with 32 per-GPU batch size.\relax }}{61}{Comparing isotropic \cnn {} and ViT. Training memory is measured on V100 GPUs with 32 per-GPU batch size.\relax }{table.caption.68}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.5}Experiments on Downstream Tasks}{61}{section.4.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Object detection and segmentation on COCO.}{61}{subsection.4.5.1}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces COCO object detection and segmentation results using Mask-RCNN and Cascade Mask-RCNN. $^\ddag  $ indicates that the model is pre-trained on ImageNet-22K. ImageNet-1K pre-trained Swin results are from their Github repository\nobreakspace  {}\cite {swindetcode}. AP numbers of the ResNet-50 and X101 models are from \cite {Liu2021swin}. We measure FPS on an A100 GPU. FLOPs are calculated with image size (1280, 800).  \relax }}{62}{table.caption.69}\protected@file@percent }
\newlabel{tab:coco}{{\M@TitleReference {4.3}{COCO object detection and segmentation results using Mask-RCNN and Cascade Mask-RCNN. $^\ddag  $ indicates that the model is pre-trained on ImageNet-22K. ImageNet-1K pre-trained Swin results are from their Github repository\nobreakspace  {}\cite {swindetcode}. AP numbers of the ResNet-50 and X101 models are from \cite {Liu2021swin}. We measure FPS on an A100 GPU. FLOPs are calculated with image size (1280, 800).  \relax }}{62}{COCO object detection and segmentation results using Mask-RCNN and Cascade Mask-RCNN. $^\ddag $ indicates that the model is pre-trained on ImageNet-22K. ImageNet-1K pre-trained Swin results are from their Github repository~\cite {swindetcode}. AP numbers of the ResNet-50 and X101 models are from \cite {Liu2021swin}. We measure FPS on an A100 GPU. FLOPs are calculated with image size (1280, 800).  \relax }{table.caption.69}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Semantic segmentation on ADE20K.}{62}{subsection.4.5.2}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces ADE20K validation results using UperNet\nobreakspace  {}\cite {Xiao2018}. $^\ddag  $ indicates IN-22K pre-training. Swins' results are from its GitHub repository\nobreakspace  {}\cite {swincode}. Following Swin, we report mIoU results with multi-scale testing. FLOPs are based on input sizes of (2048, 512) and (2560, 640) for IN-1K and IN-22K pre-trained models, respectively.\relax }}{63}{table.caption.70}\protected@file@percent }
\newlabel{tab:seg}{{\M@TitleReference {4.4}{ADE20K validation results using UperNet\nobreakspace  {}\cite {Xiao2018}. $^\ddag  $ indicates IN-22K pre-training. Swins' results are from its GitHub repository\nobreakspace  {}\cite {swincode}. Following Swin, we report mIoU results with multi-scale testing. FLOPs are based on input sizes of (2048, 512) and (2560, 640) for IN-1K and IN-22K pre-trained models, respectively.\relax }}{63}{ADE20K validation results using UperNet~\cite {Xiao2018}. $^\ddag $ indicates IN-22K pre-training. Swins' results are from its GitHub repository~\cite {swincode}. Following Swin, we report mIoU results with multi-scale testing. FLOPs are based on input sizes of (2048, 512) and (2560, 640) for IN-1K and IN-22K pre-trained models, respectively.\relax }{table.caption.70}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Remarks on model efficiency.}{63}{subsection.4.5.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.6}Related Work}{64}{section.4.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Hybrid models}{64}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Recent convolution-based approaches}{64}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.7}Full Experimental Settings}{64}{section.4.7}\protected@file@percent }
\newlabel{sec:setting}{{\M@TitleReference {4.7}{Full Experimental Settings}}{64}{Full Experimental Settings}{section.4.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}ImageNet (Pre-)training}{64}{subsection.4.7.1}\protected@file@percent }
\newlabel{subsec:setting}{{\M@TitleReference {4.7.1}{ImageNet (Pre-)training}}{64}{ImageNet (Pre-)training}{subsection.4.7.1}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces ImageNet-1K/22K (pre-)training settings. Multiple stochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model (e.g., ConvNeXt{}-T/S/B/L) respectively.\relax }}{65}{table.caption.71}\protected@file@percent }
\newlabel{tab:train_detail}{{\M@TitleReference {4.5}{ImageNet-1K/22K (pre-)training settings. Multiple stochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model (e.g., ConvNeXt{}-T/S/B/L) respectively.\relax }}{65}{ImageNet-1K/22K (pre-)training settings. Multiple stochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model (e.g., \cnn {}-T/S/B/L) respectively.\relax }{table.caption.71}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces ImageNet-1K fine-tuning settings. Multiple values (e.g., 0.8/0.95) are for each model (e.g., ConvNeXt{}-B/L) respectively. \relax }}{66}{table.caption.72}\protected@file@percent }
\newlabel{tab:ft_detail}{{\M@TitleReference {4.6}{ImageNet-1K fine-tuning settings. Multiple values (e.g., 0.8/0.95) are for each model (e.g., ConvNeXt{}-B/L) respectively. \relax }}{66}{ImageNet-1K fine-tuning settings. Multiple values (e.g., 0.8/0.95) are for each model (e.g., \cnn {}-B/L) respectively. \relax }{table.caption.72}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}ImageNet Fine-tuning}{67}{subsection.4.7.2}\protected@file@percent }
\newlabel{subsec:ft-setting}{{\M@TitleReference {4.7.2}{ImageNet Fine-tuning}}{67}{ImageNet Fine-tuning}{subsection.4.7.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Downstream Tasks}{67}{subsection.4.7.3}\protected@file@percent }
\newlabel{subsec:downstream-setting}{{\M@TitleReference {4.7.3}{Downstream Tasks}}{67}{Downstream Tasks}{subsection.4.7.3}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces ADE20K validation results with single-scale testing.\relax }}{67}{table.caption.73}\protected@file@percent }
\newlabel{tab:seg-ss}{{\M@TitleReference {4.7}{ADE20K validation results with single-scale testing.\relax }}{67}{ADE20K validation results with single-scale testing.\relax }{table.caption.73}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Detailed Architectures}{68}{subsection.4.7.4}\protected@file@percent }
\newlabel{sec:arch}{{\M@TitleReference {4.7.4}{Detailed Architectures}}{68}{Detailed Architectures}{subsection.4.7.4}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Detailed architecture specifications for ResNet-50, ConvNeXt{}-T and Swin-T.\relax }}{68}{table.caption.74}\protected@file@percent }
\newlabel{table:arch-spec}{{\M@TitleReference {4.8}{Detailed architecture specifications for ResNet-50, ConvNeXt{}-T and Swin-T.\relax }}{68}{Detailed architecture specifications for ResNet-50, \cnn {}-T and Swin-T.\relax }{table.caption.74}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces Robustness evaluation of ConvNeXt. We do not make use of any specialized modules or additional fine-tuning procedures. \relax }}{69}{table.caption.75}\protected@file@percent }
\newlabel{tab:robustness}{{\M@TitleReference {4.9}{Robustness evaluation of ConvNeXt. We do not make use of any specialized modules or additional fine-tuning procedures. \relax }}{69}{Robustness evaluation of ConvNeXt. We do not make use of any specialized modules or additional fine-tuning procedures. \relax }{table.caption.75}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.8}Additional Studies}{69}{section.4.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Robustness Evaluation}{69}{subsection.4.8.1}\protected@file@percent }
\newlabel{sec:robustness}{{\M@TitleReference {4.8.1}{Robustness Evaluation}}{69}{Robustness Evaluation}{subsection.4.8.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Modernizing ResNets: Detailed Results}{69}{subsection.4.8.2}\protected@file@percent }
\newlabel{sec:modernizing_result}{{\M@TitleReference {4.8.2}{Modernizing ResNets: Detailed Results}}{69}{Modernizing ResNets: Detailed Results}{subsection.4.8.2}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces Detailed results for modernizing a ResNet-50. Mean and standard deviation are obtained by training the network with three different random seeds.\relax }}{70}{table.caption.76}\protected@file@percent }
\newlabel{tab:modernizing-t}{{\M@TitleReference {4.10}{Detailed results for modernizing a ResNet-50. Mean and standard deviation are obtained by training the network with three different random seeds.\relax }}{70}{Detailed results for modernizing a ResNet-50. Mean and standard deviation are obtained by training the network with three different random seeds.\relax }{table.caption.76}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces Detailed results for modernizing a ResNet-200.\relax }}{71}{table.caption.77}\protected@file@percent }
\newlabel{tab:modernizing-b}{{\M@TitleReference {4.11}{Detailed results for modernizing a ResNet-200.\relax }}{71}{Detailed results for modernizing a ResNet-200.\relax }{table.caption.77}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Benchmarking on A100 GPUs}{72}{subsection.4.8.3}\protected@file@percent }
\newlabel{sec:a100}{{\M@TitleReference {4.8.3}{Benchmarking on A100 GPUs}}{72}{Benchmarking on A100 GPUs}{subsection.4.8.3}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.12}{\ignorespaces Inference throughput comparisons on an A100 GPU. Using TF32 data format and ``channel last'' memory layout, ConvNeXt{} enjoys up to $\sim $49\% higher throughput compared with a Swin Transformer with similar FLOPs.\relax }}{72}{table.caption.78}\protected@file@percent }
\newlabel{tab:a100}{{\M@TitleReference {4.12}{Inference throughput comparisons on an A100 GPU. Using TF32 data format and ``channel last'' memory layout, ConvNeXt{} enjoys up to $\sim $49\% higher throughput compared with a Swin Transformer with similar FLOPs.\relax }}{72}{Inference throughput comparisons on an A100 GPU. Using TF32 data format and ``channel last'' memory layout, \cnn {} enjoys up to $\sim $49\% higher throughput compared with a Swin Transformer with similar FLOPs.\relax }{table.caption.78}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.9}Discussions}{73}{section.4.9}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.1}Limitations}{73}{subsection.4.9.1}\protected@file@percent }
\newlabel{sec:limit}{{\M@TitleReference {4.9.1}{Limitations}}{73}{Limitations}{subsection.4.9.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.2}Societal Impact}{73}{subsection.4.9.2}\protected@file@percent }
\newlabel{sec:impact}{{\M@TitleReference {4.9.2}{Societal Impact}}{73}{Societal Impact}{subsection.4.9.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.10}Conclusions}{73}{section.4.10}\protected@file@percent }
\@setckpt{chap4}{
\setcounter{page}{75}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{-1}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{10}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{1}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{91}
\setcounter{lastsheet}{103}
\setcounter{lastpage}{87}
\setcounter{figure}{4}
\setcounter{lofdepth}{1}
\setcounter{table}{12}
\setcounter{lotdepth}{1}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{510}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{7}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{r@tfl@t}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{bookmark@seq@number}{94}
\setcounter{nlinenum}{0}
\setcounter{section@level}{1}
}
